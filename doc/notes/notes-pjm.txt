# ============================================================================
# Shortcuts to useful snippets, for reproducing the SW results:
# goto CFHTLS_STAGE1
# goto SPREADCHECK
goto FNFPTRAJECTORIES
# ============================================================================
# 2013-04-07 (Sunday) 00:58 IST
#
# Get beta click database and start writing analysis code for it.

cd analysis/workspace

wget http://spacewarps.org.s3.amazonaws.com/data-export/spacewarps-2013-04-06_20-07-28.tar.gz

tar xvfz spacewarps-2013-04-06_20-07-28.tar.gz

ls spacewarps-2013-04-06_20-07-28/ouroboros/
# spacewarp_classifications.bson          spacewarp_subjects.bson
# spacewarp_classifications.metadata.json spacewarp_subjects.metadata.json

# These are files to be read into a mongoDB, using mongodbrestore.

# Needs pymongo... pip install? Yep - done

# Also need binary installation of mongodb, from mongodb.org
# Downloaded to software, copied to usr/local/bin

mongorestore spacewarps-2013-04-06_20-07-28
# Sun Apr  7 01:23:48.133 kern.sched unavailable
# couldn't connect to [127.0.0.1] couldn't connect to server 127.0.0.1:27017

# Hmm. Looks beyond my pay grade.
# Try from python:

cd spacewarps-2013-04-06_20-07-28/
python ../../swap/mongodb.py

Traceback (most recent call last):
  File "../../swap/mongodb.py", line 23, in <module>
    m = MongoDB()
  File "../../swap/mongodb.py", line 13, in __init__
    self.client = MongoClient('localhost', 27017)
  File "/usr/local/Cellar/python/2.7.2/lib/python2.7/site-packages/pymongo/mongo_client.py", line 336, in __init__
    raise ConnectionFailure(str(e))
pymongo.errors.ConnectionFailure: could not connect to localhost:27017: [Errno 61] Connection refused

# Need to start mongodb server in the background:

mongod --dbpath . &

# Now python runs ok:

python ../../swap/mongodb.py
# Collection(Database(MongoClient('localhost', 27017), u'ouroboros'), u'spacewarp_subjects.0')
# Collection(Database(MongoClient('localhost', 27017), u'ouroboros'), u'spacewarp_classifications.0')

# What are these things?!
# OK, open up python session and start playing.

>>> import swap
>>> m = swap.MongoDB()
>>> m.subjects
Collection(Database(MongoClient('localhost', 27017), u'ouroboros'), u'spacewarp_subjects')
>>> m.subjects.find()
<pymongo.cursor.Cursor object at 0x101220f50>

# Interesting.

>>> m.subjects.find_one({'zooniverse_id' : 'ASW0000002'})
{u'project_id': ObjectId('5101a1341a320ea77f000001'),
u'classification_count': 523, u'created_at': datetime.datetime(2013, 4,
4, 20, 29, 18, 949000), u'random': 0.5465496445414518, u'updated_at':
datetime.datetime(2013, 4, 4, 20, 29, 18, 975000), u'state':
u'complete', u'trending': 3, u'coords': [], u'location': {u'thumbnail':
u'', u'standard': u''}, u'zooniverse_id': u'ASW0000002',
u'workflow_ids': [ObjectId('5101a1361a320ea77f000002')], u'_id':
ObjectId('5101a1931a320ea77f000004'), u'tutorial': True, u'metadata':
{}}

# OK cool - here's a clue to the subject schema, and the metadata on subject
# ASW0000002.

# How about a non-tutorial one?
>>> m.subjects.find_one({'zooniverse_id' : 'ASW0000004'}) {u'_id':
ObjectId('515de29fe4bb216427000001'), u'classification_count': 13,
u'created_at': datetime.datetime(2013, 4, 4, 20, 29, 19, 372000),
u'activated_at': datetime.datetime(2013, 4, 4, 20, 32, 51, 886000),
u'updated_at': datetime.datetime(2013, 4, 4, 20, 29, 19, 398000),
u'random': 0.1047423015104475, u'project_id':
ObjectId('5101a1341a320ea77f000001'), u'state': u'active',
u'zooniverse_id': u'ASW0000004', u'workflow_ids':
[ObjectId('5101a1361a320ea77f000002')], u'location': {u'thumbnail':
u'http://www.spacewarps.org/subjects/thumbnail/CFHTLS_002_0530_gri.png',
u'standard':
u'http://www.spacewarps.org/subjects/standard/CFHTLS_002_0530_gri.png'},
u'group_id': ObjectId('5154a3783ae74086ab000001'), u'metadata': {u'id':
u'CFHTLS_002_0530'}}

# Nice! There's the actual image name, from Anu. And this URL is open -
# volunteers can grab those any time if they want to model them elsewhere.
# Just need to expose that on the site somewhere.

# Anyway, what about classifications?

>>> m.classifications.find_one()
{u'_id': ObjectId('515de3a9390c056085000416'), u'created_at':
datetime.datetime(2013, 4, 4, 20, 33, 45), u'updated_at':
datetime.datetime(2013, 4, 4, 20, 33, 45, 885000), u'user_ip':
u'163.1.174.106', u'workflow_id': ObjectId('5101a1361a320ea77f000002'),
u'subjects': [], u'subject_ids': [ObjectId('515dd45de4bb21597c00026c')],
u'project_id': ObjectId('5101a1341a320ea77f000001'), u'annotations':
[{u'finished_at': u'Thu, 04 Apr 2013 20:32:19 GMT', u'started_at': u'Thu, 04
Apr 2013 19:57:45 GMT'}, {u'user_agent': u'Mozilla/5.0 (Macintosh; Intel Mac
OS X 10_7_5) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.43
Safari/537.31'}]}

>>> m.classifications.find({'_id':'515de3a9390c056085000416'})
# <pymongo.cursor.Cursor object at 0x10122c050>

# Hmm. A cursor seems to be some sort of cursor. Might need a bit of help
# learning how this works!

# Plan:
#
# Make a list of operations that I want to do,
# and ask for a suggested example command from Amit?
#
# 1) Return a list of users as an array of strings
#
# 2) For a given user, return the IDs of all the training subjects they classified
#     Presumably this array will be time-ordered?
#
# 3) For a given user and training subject ID, return the value 1 if they succeeded in recognising it, 0 if not

# Issued.


# 2013-04-18 (Thursday) 12:04 BST
#
# datetime objects are interesting! See http://docs.python.org/2/library/datetime.html
#
# >>> import datetime
# >>> print datetime.datetime(2013, 4, 4, 20, 29, 18, 949000)
# 2013-04-04 20:29:18.949000
# >>> t1 = datetime.datetime(2013, 4, 4, 20, 29, 18, 949000)
# >>> t2 = datetime.datetime(2013, 4, 5, 8, 15, 23, 543000)
# >>> t2 - t1
# datetime.timedelta(0, 42364, 594000)
# >>> t1 > t2
# False
# >>> t1 <= t2
# True

# Perfect!

# Note [2014-06-12 (Thursday) 09:07 PDT]
# - spacewarp* directory can be removed once mongo directory is populated
#   The data is ingested and reformated as a mongo database, and then the
#   source json and bson files are not used. This is important, as
#   the spacewarp* directories can run to tens of Gb.


# ============================================================================
# 2013-04-22 (Monday) 23:24 BST

# Getting data out of mongodb is not so intuitive. Wrapped it up in
# python so that I can just do, eg

cd workspace
python ../swap/mongodb.py

# But I cannot do even the most basic things - so I've written down what
# I want to do in plausible python, for Amit to advise on!

# ============================================================================
# 2013-04-24 (Wednesday) 10:18 BST
#
# Great session with Amit yesterday solving all my db access problems!
# He says that on the Zooniverse system, the correct db is always online,
# so we should just connect to the client. That means that all of this
# wrapper I wrote is redundant - but I preserve it here, for reference:

#     self.dumpname = dumpname
#
#     # Keep a record of what goes on:
#     self.logfilename = os.getcwd()+'/'+self.dumpname+'.log'
#     self.logfile = open(self.logfilename,"w")
#
#     # Start the Mongo server in the background:
#     subprocess.call(["mongorestore",self.dumpname],stdout=self.logfile,stderr=self.logfile)
#     os.chdir(self.dumpname)
#     self.cleanup()
#     self.process = subprocess.Popen(["mongod","--dbpath","."],stdout=self.logfile,stderr=self.logfile)
#
#     # Check everything is working:
#     if self.process.poll() == None: print "MongoDB: server is up and running"
#
#     # Connect to the Mongo:
#     self.client = MongoClient('localhost', 27017)
#     self.db = self.client['ouroboros']
#
#     self.subjects = self.db['spacewarp_subjects']
#     self.classifications = self.db['spacewarp_classifications']

# Followed by, later,
#     m.terminate()

# Worked example gives the following result, from the initial beta DB:

# Counted  7564  classifications, that each look like:
# ('2013-04-06 19:54:42.146000', '5065ff08d10d244cd10029ba', '515de2efe4bb21642700019a', 'test', 'NOT')

# ============================================================================
# 2013-04-29 (Monday) 17:03 BST

# OK, got SWAP working in its simplest incarnation.
# REady to do some experiments!

# Standard setup is this on: agents learn, but set PD=PL=50%, initially.
SWAP.py CFHTLS-beta_P50.config

# Aprajita asks, do we have to learn? Why not assume one set of P's and
# stick with them? Get off to a fsater start?
SWAP.py CFHTLS-beta_no-learning_P50.config
# Random classifiers get nowhere!

SWAP.py CFHTLS-beta_no-learning_P90.config
# Big jumps up and down in probability, lots of false positives.


# OK, so back to learning. How about we be less pessimistic about people's
# talents?
SWAP.py CFHTLS-beta_P90.config
# Not bad - Christmas tree is broader. FP rate higher, but got some sims in
# the detection zone now.

# More measured. Crowd divided between two groups?
#    The Herd: PD = 0.9, PL = U(0:1)
# Enthusiasts: PD = U(0.2:0.6), PL = U(0.6:1.0)
SWAP.py CFHTLS-beta_P70.config


# One issue with this LENS or NOT analysis is the hard edged estimation of PD
# and PL. Early classifications can have a significant impact... Hmm. 70-70
# seems like a reasonable starting point.


# Wishlist for mock survey:

# - Difficulty variation              DONE
# - PD, PL capped at 0.99             DONE
# - Realistic PDFs for Nc, PD, PL     DONE

# ============================================================================
# 2013-05-06 (Monday) 12:40 BST

# Wishlist for mock survey (continued):

# - Completeness, purity calculated   DONE 2013-05-06
# - Detected/rejected subjects        DONE 2013-05-06
# - Retired/promoted subjects         DONE 2013-05-06
# - Candidate IDs output              DONE 2013-05-06

# OK, great! Now we just need to be able to do low cost
# batch processing.


# ============================================================================
# 2013-05-07 (Tuesday) 09:43 BST

# Testing new database!

# Download:

set url = "https://zooniverse-code.s3.amazonaws.com/databases/2013-05-07/ouroboros_projects/spacewarp_2013-05-07.tar.gz?AWSAccessKeyId=AKIAJHHZ7KLFECQKTS7A&Expires=1368548396&Signature=as2nCaTgD9ctFLIHZrxj%2FrnQJoo%3D"

set dbfile = `echo "$url" | cut -d '?' -f1 | cut -d'/' -f7`
set logfile = ${dbfile:r:r}.wget.log
wget -O $dbfile "$url" >& $logfile

# Unpack:
tar xvfz $dbfile

set db = $dbfile:r:r

# First need to kill any old servers:
set pid = `ps -e | grep 'mongod --dbpath' | \
                   grep -v 'grep' | head -1 | awk '{print $1}'`
if ($#pid > 0) kill $pid

# Start new mongo server in its own directory, out of the way:
mkdir -p mongo
chdir mongo
mongod --dbpath . &
chdir ..

# Now restore the new database:
mongorestore --drop --db ouroboros $db

# Probably should script this? Maybe?


# Try running SWAP:

SWAP.py CFHTLS-test.config >& CFHTLS-test.log

# SWAP: interpreting classifications...
# SWAP: total no. of classifications processed:  0


# Scripted! Just need to download db tarball manually:

SWIPE.csh spacewarp_2013-05-07.tar.gz


# ============================================================================
# 2013-05-07 (Tuesday) 18:36 BST

# Control of retirement from afar! Coooooool.

# Email from Michael Parrish:


# Hi Phil,
#
# I'm attaching an example administration client script (in python) along
# with the documentation for administration endpoints. The script depends
# on the requests library, which you can install with `pip install
# requests`.
#
# The admin account in the script is active -- feel free to
# retire/activate some subjects to make sure it's working for you.
#
# Since this is all new code, there may still be some rough edges to work
# through, so please let me know if you have questions/problems.
#
# -Michael
#
#
#
# Administration:
#
# Administration requests are authenticated with a combination of a
# password, a private key, a public key, and a sequence identifier.
#
#
# Administrators:
#
# Accounts are created by request. A password, and private key will be
# supplied.
#
#
# Session:
#
# A session begins by logging in with a POST request to /admin/login with
# { name: 'your admin name', password: 'your admin password' }
#
# A successful login responds with a new public_key.
#
#
# Requests:
#
# After establishing a session, requests can be made by signing each
# request with the name of the administrator and request key.
#
# Request keys are generated by combining the public and private keys as
# well as a sequence identifier.
#
# At the beginning of a session, the sequence identifier is 1. Each
# request increments this number.
#
# In the case of an unauthorized request, sequence identifiers will no
# longer match causing the session to terminate.
#
# If a session terminates, a new login request must be sent to restart the
# session with a new public key.
#
#
# Login Limits:
#
# More than 10 unsuccessful login attempts within an hour will temporarily
# block logins by the administrator account. More than 100 unsuccessful
# login attempts within an hour will disable the administrator account. A
# developer will have to reactivate it.
#
# Requests:
#
# Administration requests are rate limited to less than 1,000 per hour.
# All requests, successful or not, are logged for security purposes.
#
# Example code:
#
# Subject Administration
#
# Method  Action      Path                                                Params
# PUT     activate    /admin/projects/:project_id/subjects/:id/activate   { }
# PUT     retire      /admin/projects/:project_id/subjects/:id/retire     { }
# PUT     pause       /admin/projects/:project_id/subjects/:id/pause      { }
# PUT     resume      /admin/projects/:project_id/subjects/:id/resume     { }


# OK, got it! Wrote SWITCH.py to read in a list of subject IDs, and
# then make put requests to retire them.

# First run SWAP with low rejection threshold to get a bunch of
# retirements to do:

SWAP.py CFHTLS-SWITCH-test.config

# SWAP: interpreting classifications...
# SWAP:
# SWAP: total no. of classifications processed:  37
# SWAP: saving newly retired subject IDs...
# SWAP: 19 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS-SWITCH-test_2013-05-07/CFHTLS-SWITCH-test_2013-05-07_retire_these.txt

# Good - now SWITCH them:

set retirees = /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS-SWITCH-test_2013-05-07/CFHTLS-SWITCH-test_2013-05-07_retire_these.txt

# SWITCH: retiring subjects listed in  /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS-SWITCH-test_2013-05-07/CFHTLS-SWITCH-test_2013-05-07_retire_these.txt
# SWITCH: looks like we have 19  subjects to retire
# SWITCH: doing a dry run
# result = client.put('/projects/spacewarp/subjects/ASW000075q/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000arx/retire')
# result = client.put('/projects/spacewarp/subjects/ASW00005js/retire')
# result = client.put('/projects/spacewarp/subjects/ASW000082p/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000hlt/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000by9/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000a4m/retire')
# result = client.put('/projects/spacewarp/subjects/ASW00008wq/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000wlr/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000c6t/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000w51/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000kx4/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000s3v/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000l4y/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000d6r/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000b6n/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000tc7/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000hs9/retire')
# result = client.put('/projects/spacewarp/subjects/ASW0000kig/retire')

# OK, looks good - give it a try!

SWITCH.py $retirees

# Whoah - success?
#
# SWITCH: retiring subjects listed in  /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS-SWITCH-test_2013-05-07/CFHTLS-SWITCH-test_2013-05-07_retire_these.txt
# SWITCH: looks like we have 19  subjects to retire
#
# ...
#
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(0)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(1)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(2)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(3)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(4)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(5)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(6)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(7)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(8)
# 'instancemethod' object is not subscriptable(0)
# 'instancemethod' object is not subscriptable(1)
# 'instancemethod' object is not subscriptable(2)
# 'NoneType' object is not iterable(9)
# ASW0000kig False
# ======================================================================

# Check with Michael!! :-)

# Yep, all good after changing response.json to response.json()

# ======================================================================
# 2013-05-08 (Wednesday) 14:29 BST

# Launch day! :-)
# First batch of classifications from Michael. First tidy away all the
# beta work... and also the SWITCH test:

mkdir -p Beta
mv *beta* Beta/
mv spacewarps-2013-04-06_20-07-28* Beta/

mkdir -p SWITCH-test
mv *-SWITCH* SWITCH-test/
mv spacewarp_* SWITCH-test/
mv CFHTLS* SWITCH-test/

# Now make a working directory:
mkdir -p 2013-05-08
cd 2013-05-08

# OK, get new db:

set url = "https://zooniverse-code.s3.amazonaws.com/databases/2013-05-08/ouroboros_projects/spacewarp_2013-05-08.tar.gz?AWSAccessKeyId=AKIAJHHZ7KLFECQKTS7A&Expires=1368612013&Signature=QRK%2BOV37H6khTqgqoTu0z%2FLO%2F2s%3D"

set dbfile = `echo "$url" | cut -d '?' -f1 | cut -d'/' -f7`
set logfile = ${dbfile:r:r}.wget.log
wget -O $dbfile "$url" >& $logfile

# OK, SWIPE this:

SWIPE.csh $dbfile

# Good! This script should write a config file.

SWAP.py startup.config

# Nice - 10,000 classifications of 8000 subjects by 300 people. Crowd
# look very similar to beta! Result :-)

# ======================================================================
# 2013-05-09 (Thursday) 09:46 BST

# Gosh - half a million classifications in the first day!
# Better get this code working at scale - and fast.

# OK, SWAPSHOP is operational and checked in.

# Run on 1st 300,000 classifications!
# Just did:

SWAPSHOP.csh

# Got 117 candidates and 131 false negatives from first 300,000
# classifications, in 2013-05-09

# Made plot of just false negatives for AV - quite some uncertainty,
# subjects move back and forth quite a lot. Agents are overconfident!


# Try shifting thresholds to 0.4 and 1e-7 (symmetrical). How many
# retirements? Completeness stats?

# Edited swap/startup.config, then ran:

time SWAPSHOP.csh -s CFHTLS-strict -f --fast

# --fast is no animation, so no plots until the end!

# Looks like its about 20s per batch, increasing with time though.
# 11 batches total, plus plots, takes 8.5 mins.

# STill getting 17% false negatives, and with only 18000 retirements!


# Try a cheap version of Surhud's idea - ignore the first few clicks?
# Coded as ignore if agent.NT <= a_few_at_the_start where this
# refers to a few training images. Lets try an extreme version with
# a_few_at_the_start = 10!

time SWAPSHOP.csh -s CFHTLS-reallystrict -f --fast

# Same number of classifiers/agents, but now many will stay at 0.5
# because they never leave... 10 training images requires about 30
# images total in the standard stream. The agent history only gets
# updated for classifications that count

# OK, I get 11500 retirements, and down to 13% false negatives
# The total number of classifications is now 267000, so we lose about
# 100k to the traiing programme! Candidates are still at 96, as with
# strict routine.

# Look at the images! Well, ones that are still negative:

mkdir training_false_negatives
set repo = ../2013-05-09/training_false_negatives

foreach url ( `cat CFHTLS-reallystrict_2013-05-09_01:38:24/CFHTLS-reallystrict_2013-05-09_01:38:24_training_false_negatives.txt` )
    set png = $url:t
    cp $repo/$png training_false_negatives/.
end

# OK, we had all but 3 already.
# cp: ../2013-05-09/training_false_negatives/5183f151e4bb2102190044e2.png: No such file or directory
# cp: ../2013-05-09/training_false_negatives/5183f151e4bb210219007187.png: No such file or directory
# cp: ../2013-05-09/training_false_negatives/5183f151e4bb210219004b03.png: No such file or directory

# What do *these* look like?

# Mostly LRGs with rings hidden in depths. All fairly tricky.
# ======================================================================
# 2013-05-10 (Friday) 12:17 BST

# New db! Try analysing with the really straict settings that are
# currently checked in:

SWIPE.csh spacewarp_2013-05-10.tar.gz

time SWAPSHOP.csh -s CFHTLS-reallystrict -f --fast

# Hmm - weird. 1.08 million classifications, but report only shows
# 396281! Still, 15% lenses missed, 82% completeness, 99.8% purity
# in sims vs duds. 33078 subjects to retire. Leave the candidates in
# to be inspected more often! FPs are not a problem.

# Try running without the 10-subject training period.

time SWAPSHOP.csh -s CFHTLS-strict -f --fast

# OK, hmm. Seems I am counting wrong!
# Yep - switched to agents counting all classifications,
# but subjects only counts that matter to them.

# OK, last tests: try really strict with 95% detection threshold,
# and retirement only at low probability end.

time SWAPSHOP.csh -s CFHTLS-reallystrict95 -f --fast -d

# Affects number of detections, but not enormously they are all highly
# classified at this point


# Talked to Amit about using simFound annotation for sims!
# Implemented, lets compare with strict (reset thresholds):

time SWAPSHOP.csh -s CFHTLS-strict-simFound -f --fast

# Hmm - looks very odd. Save and investigate...
# Put in a print statement in mongodb.digest - what's going on?

time SWAPSHOP.csh -s CFHTLS-strict-simFound -f --fast -t 1

# OK, ran with verbose and one_by_one. Results seem plausible,
# but there aren't too many hits on the sims!

# In db.digest: kind,N_markers,simFound,result,truth =  sim 0 false NOT LENS
# SWAP: --------------------------------------------------------------------------
# SWAP: Subject 5183f151e4bb210219005ac5 was classified by 63.250.229.206
# SWAP: he/she said NOT when it was actually LENS
# SWAP: their agent reckons their contribution (in bits) =  1.99471942959
# SWAP: while estimating their PL,PD as  0.152173913043 0.933333333333
# SWAP: and the subject's new probability as  0.000159955385968

# NB information is wrong...
# If accuracy is missing, then we get a bunch of people just saying
# no to everything! And then subjects just fall straight.


# Try initialNL = 5 or so to allow for mistakes early on, and compare
# use_marker_positions = True and False, on the first 300k.

time SWAPSHOP.csh -s CFHTLS-strict-NO-useXY -f --fast -t 6

# and then:

time SWAPSHOP.csh -s CFHTLS-strict-YES-useXY -f --fast -t 6

# OK! Compare numbers:

# Very similar, but:
#
# Quantity        NO     YES
# Retirements    3000    150
# Missed lenses   13%     9%

# ie the bureau is more circumspect. And much slower to retire!

# DECISION: use XY, but investigate skepticism. These results used
# skepticism = 3! Also, need more data. Run on all of 2010-05-10.

# Now, vary agent skepticism (initialNL,ND = 2 + skepticism).
# Also, bump up detection threshold to 0.95!

time SWAPSHOP.csh -s CFHTLS-skeptic00 -f --fast

time SWAPSHOP.csh -s CFHTLS-skeptic03 -f --fast

time SWAPSHOP.csh -s CFHTLS-skeptic08 -f --fast

# Skepticism              0        3        8
# Retirements         28489    25574    21271
# Age at retirement    15.6     17.8     20.3
# Missed lenses        18.5%    18.3%    18.1%
# Candidates            148       64       21

# Looks like skepticism slows things down without affecting false -ve
# rate very much. I think we should go with skepticism = 0.

# Using 15 classifications per retirement is expensive.
# 400,000 * 15 = 6 million classifications!
# Maybe not that many: 375 each from 16,000 people

# Trajectories are roughly horizontal by the time we get close to
# threshold - good, there is an end in sight!


# What if using marker positions is causing a high false negative rate
# somehow? By giving people low PL values, so that they are disbelieved?
# Try skeptic00 with marker positions turned off.

time SWAPSHOP.csh -s CFHTLS-skeptic00-noXY -f --fast

# BTW each full run (10^6 classifications) takes about 14 mins.

# Use XY positions?     Yes       No
# Retirements         28489    35835
# Age at retirement    15.6     10.7
# Missed lenses        18.5%    20.1%
# Candidates            148      159

# So XY gives a *better* false neg rate! Good.

# So, what about a training period? Level 1 has 20 subjects, 1 in 5 sims
# and 1 in 5 duds, so 8 training images total. Try ignoring all of level
# 1 and see what we get compared to the standard setup.

time SWAPSHOP.csh -s CFHTLS-skeptic00-ignore8 -f --fast -t 20

# (Note that I SWIPED the new db in the meantime - oops)

# Run:                skeptic00      ignore8
# Classifications        666864       536099
# Retirements             28489        27346
# Age at retirement        15.6         12.8
# Missed lenses            18.5%        16.3%
# Candidates                148          146

# So its cleaner and cheaper. I think we have to do this. Make it
# past level 2 and your classifications start to count! Rough
# qualification is 20 classifications - we asked for 40 in the PR.
# We can always go back later and make more use of the ignored clicks,
# with a better model for how the agents learn.

# OK, done! Retirement plan is skeptic00, defined by this config:
#
# skepticism: 0
#
# a_few_at_the_start: 8
#
# use_marker_positions: True
#
# detection_threshold: 0.95
#
# rejection_threshold: 1e-7
#
# Checked in as startup.config! Done.

# ======================================================================
# 2013-05-11 (Saturday) 19:52 BST

# Restore latest db to start production run. Run SWAPSHOP in production
# directory so that we can re-use the pickles.

mkdir -p $SWAP_DIR/analysis/production
chdir $SWAP_DIR/analysis/production

SWIPE.csh spacewarp_2013-05-11.gz

# OK, now SWAPSHOP with simple survey name. No need for -f once we are
# rolling!

SWAPSHOP.csh -s CFHTLS -f --fast

# OK good - we are rolling!

# ======================================================================
# 2013-05-12 (Sunday) 19:57 CEST

# Right, more retirements!

SWIPE.csh spacewarp_2013-05-12.gz

SWAPSHOP.csh -s CFHTLS --fast

# Had to do retirments by hand, as script got it wrong to start with.
# Here's how I recovered:

wc -l */*retire*
#    33648 CFHTLS_2013-05-11_10:04:13/CFHTLS_2013-05-11_10:04:13_retire_these.txt
#    36706 CFHTLS_2013-05-12_10:05:01/CFHTLS_2013-05-12_10:05:01_retire_these.txt

set survey = CFHTLS
\set latest = `\ls -dtr ${survey}_????-??-??_??:??:?? | tail -1`

set previousretirees = CFHTLS_previously_retired.txt
cat $previousretirees         | sort > old
cat $latest/*retire_these.txt | sort > new
sdiff -s old new | & cut -d'>' -f2 > diff
wc -l old new diff
#    33648 old
#    36706 new
#     3058 diff
mv diff CFHTLS_production_retire_these.txt

# Good! Retire these:

SWITCH.py CFHTLS_production_retire_these.txt

# ================================================================================
#                    SWITCH: the Space Warps Retirement Plan
# ================================================================================
# SWITCH: retiring subjects listed in  CFHTLS_production_retire_these.txt
# SWITCH: looks like we have 3058  subjects to retire
# SWITCH: successfully retired subject ASW0000008
# SWITCH: successfully retired subject ASW000000t
# SWITCH: successfully retired subject ASW0000016
# SWITCH: successfully retired subject ASW000001h
# SWITCH: successfully retired subject ASW000001i
# SWITCH: successfully retired subject ASW000001k
# SWITCH: successfully retired subject ASW000001o

# etc . Success!

# Well, partially. Got back fom dinner to find:

# SWITCH: retirement fail:  ASW0000chb False
# failed: unauthorized, check your credentials(0)

# for a whole bunch of subjects. Emailed Parrish for advice.

# ======================================================================
# 2013-05-14 (Tuesday) 00:54 CEST

# OK, I hit the retirements per hour limit. Michael put it up to 5000
# That means we can do 1 retirment every 0.72 secs. Add sleep 0.7 to
# code, and run overnight! We probably only did 3000 yesterday before it
# got locked out, so might as well start from teh beginning gain.

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Great, now they are all done!

# ======================================================================
# 2013-05-15 (Wednesday) 00:38 CEST

# Catching up - need to do both 13 and 14 May dumps.

SWIPE.csh spacewarp_2013-05-13.gz
SWAPSHOP.csh -s CFHTLS --fast

# Got 7463 subjects to retire - do in 3 batches:

head -2500 CFHTLS_production_retire_these.txt > batch1.txt
head -5000 CFHTLS_production_retire_these.txt | tail -2500 > batch2.txt
tail -2463 CFHTLS_production_retire_these.txt > batch3.txt

SWITCH.py batch1.txt > retirement1.log &

# Do the following next!! 2013-05-15 (Wednesday) 09:36 CEST

SWITCH.py batch2.txt > retirement2.log &
SWITCH.py batch3.txt > retirement3.log &

# Phew - just finished. That was an effort. 2013-05-17 (Friday) 01:04 CEST

# Tomorrow, need to process 2013-05-14, 15, 16... Starting with SWIPE.

# ======================================================================
# 2013-05-17 (Friday) 18:51 CEST

# On plane home from Copenhagen. Catch up!
# Need to deal with interrupted retirements in SWITCH.

# First, run SWAPSHOP on latest db! Make plots later...

SWIPE.csh spacewarp_2013-05-17.gz
SWAPSHOP.csh -s CFHTLS --fast

# SWAP: report compiled as /Users/pjm/public_html/SpaceWarps/Science/analysis/production/CFHTLS_2013-05-17_10:07:53/CFHTLS_2013-05-17_10:07:53_report.pdf
# ================================================================================
# SWAPSHOP: if you want, you can go ahead and retire 40595 subjects with
#
#           SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Report shows 84130 subjects, 63722 to be retired.
# Numbers don't add up! Check lists:

sort -n CFHTLS_previously_retired.txt | uniq | sed s/' '//g | grep 'ASW' > old
sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new

tabs2spaces -n 0 old new

wc -l CFHTLS_previously_retired.txt old CFHTLS_production_retire_these.txt new
#    44169 CFHTLS_previously_retired.txt
#    44169 old
#    40595 CFHTLS_production_retire_these.txt
#    33132 new

# Retirement list has some repeats in it!
# Also, sdiff does not work for finding difference between files.
# Brute force it! First, update these files.

mv new CFHTLS_production_retire_these.txt
rm old

\rm junk
foreach ID ( `cat CFHTLS_production_retire_these.txt` )
  set done = `grep $ID CFHTLS_previously_retired.txt | wc -l`
  if (! $done) echo $ID >> junk
end
wc -l junk
#   25669 junk

# OK, 44169 old + 25669 new = 69838
# Why is this different from the 63722 listed?
# Did many of the old ones not actually get retired?
# Safe thing to do is retire eveything in
# CFHTLS_production_retire_these.txt - won't make any difference to
# re-retire things, it just takes longer. Run overnight!

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &


# ======================================================================
# 2013-05-19 (Sunday) 09:35 BST

# Run SWAPSHOP on latest db. Hopefully retirements are now cleaned up
# following yesterday's run:

SWIPE.csh spacewarp_2013-05-18.gz
SWAPSHOP.csh -s CFHTLS --fast

# SWAPSHOP: if you want, you can go ahead and retire 15228 subjects with
#
#           SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Hmm - still not counting right?

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#     7765 new

# Hmm. Need to fix this. Over-write for now!

mv new CFHTLS_production_retire_these.txt

# Pausing retirment plan while we wait for Michael...

# 2013-05-20 (Monday) 23:03 BST
# OK got green light!

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# OK good! Onto May 20.
# 2013-05-21 (Tuesday) 10:54 BST

SWIPE.csh spacewarp_2013-05-21.gz
SWAPSHOP.csh -s CFHTLS --fast

# Wow - 32517 retirements?!

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new

# OK, "only" 17289 really...

mv new CFHTLS_production_retire_these.txt

# Retire them!

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-22 (Wednesday) 13:24 BST

SWIPE.csh spacewarp_2013-05-22.gz
SWAPSHOP.csh -s CFHTLS --fast
sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   19193 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-23 (Thursday) 12:45 BST

SWIPE.csh spacewarp_2013-05-23.gz
SWAPSHOP.csh -s CFHTLS --fast
sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   23533 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-24 (Friday) 12:17 BST

SWIPE.csh spacewarp_2013-05-24.gz
SWAPSHOP.csh -s CFHTLS --fast
sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#
mv new CFHTLS_production_retire_these.txt

# Try retiring from SLAC: Fail! No requests module. Ho hum.

nohup SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-26 (Sunday) 21:00 BST

SWIPE.csh spacewarp_2013-05-26.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   44392 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-05-27 (Monday) 17:44 BST

SWIPE.csh spacewarp_2013-05-27.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   49761 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Wow - that won't leave many...

# ======================================================================
# 2013-05-26 (Sunday) 21:00 BST

SWIPE.csh spacewarp_2013-05-28.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#    54393 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Need to get D6 in? Wait for a bit.

# ======================================================================
# 2013-05-30 (Thursday) 09:59 BST

SWIPE.csh spacewarp_2013-05-29.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#  58484 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Retire these duds!

# Ran out of time - had to abort:
wc -l retirement.log
#   28238 retirement.log

# Restart on Friday night from New York!

# ======================================================================
# 2013-05-31 (Friday) 23:50 EDT

SWIPE.csh spacewarp_2013-05-31.gz
SWAPSHOP.csh -s CFHTLS --fast

# SWAP: interpreting up to 50000  classifications...
# ERROR: AttributeError: 'NoneType' object has no attribute 'has_key' [swap.mongodb]
# Traceback (most recent call last):
#   File "/Users/pjm/public_html/SpaceWarps/Science/analysis/SWAP.py", line 422, in <module>
#     SWAP(sys.argv[1:])
#   File "/Users/pjm/public_html/SpaceWarps/Science/analysis/SWAP.py", line 203, in SWAP
#     items = db.digest(classification,method=use_marker_positions)
#   File "/Users/pjm/public_html/SpaceWarps/Science/analysis/swap/mongodb.py", line 154, in digest
#     if subject.has_key('group_id'):
# AttributeError: 'NoneType' object has no attribute 'has_key'
# SWAPSHOP: if you want, you can go ahead and retire 140567 subjects with
#
#           SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Hmm - wonder what that problem is?

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#    58484 new
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Off we go again! Run overnight.

# ======================================================================
# 2013-06-01 (Saturday) 22:18 EDT

SWIPE.csh spacewarp_2013-06-01.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   73358 new
mv new CFHTLS_production_retire_these.txt

# Seems very high! Are retirement requests not working?
# Compare log with new list:

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored"
echo "List of subjects to be retired afresh:"
wc -l new

# 58484 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#   14874 new

mv new CFHTLS_production_retire_these.txt

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-03 (Monday) 08:16 EDT

SWIPE.csh spacewarp_2013-06-03.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#   65263 new
mv new CFHTLS_production_retire_these.txt

# Seems very high! Are retirement requests not working?
# Compare log with new list:

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored"
echo "List of subjects to be retired afresh:"
wc -l new

# 0 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
# wc -l new
#    65263 new

# Wow.
# OK, better get started! :-)

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Stop part way through:

grep -n ASW0001jfc CFHTLS_production_retire_these.txt
# 12195:
tail -n +12196 CFHTLS_production_retire_these.txt > \
               CFHTLS_production_now_retire_these.txt

SWITCH.py CFHTLS_production_now_retire_these.txt >> retirement.log &

# And again!

grep -n ASW000261b CFHTLS_production_retire_these.txt
# 30106:
tail -n +30107 CFHTLS_production_retire_these.txt > \
               CFHTLS_production_and_now_retire_these.txt

SWITCH.py CFHTLS_production_and_now_retire_these.txt >> retirement.log &

# And again...

grep -n ASW0002lsr CFHTLS_production_retire_these.txt
# 45799:
tail -n +45800 CFHTLS_production_retire_these.txt > \
               CFHTLS_production_and_NOW_retire_these.txt

SWITCH.py CFHTLS_production_and_NOW_retire_these.txt >> retirement.log &


# ======================================================================
# 2013-06-05 (Wednesday) 07:45 EDT

SWIPE.csh spacewarp_2013-06-05.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
#  72222 new
mv new CFHTLS_production_retire_these.txt

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored"
echo "List of subjects to be retired afresh:"
wc -l new

# 65260 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#     6959 new

mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Got a bunch of these:
#
# HTTPSConnectionPool(host='api.zooniverse.org', port=443): Max retries exceeded with url: /admin/login (Caused by <class 'socket.error'>: [Errno 50] Network is down)(1)

# Killed process. Restart:

grep ASW retirement.log | grep -v fail | grep -v error | tail -1
# SWITCH: successfully retired subject ASW0004weo
grep -n ASW0004weo CFHTLS_production_retire_these.txt
# 5904:ASW0004weo
tail -n +5905 CFHTLS_production_retire_these.txt > \
              CFHTLS_production_now_retire_these.txt

SWITCH.py CFHTLS_production_now_retire_these.txt >> retirement.log &

# ======================================================================
# 2013-06-06 (Thursday) 23:01 CDT

SWIPE.csh spacewarp_2013-06-06.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 70950  Pretty sure these are not all new but retire them anyway...
mv new CFHTLS_production_retire_these.txt
SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# Killed job in order to start fresh on Sunday!

grep uccess retirement.log | wc -l
#    58595
# ======================================================================
# 2013-06-09 (Sunday) 09:46 PDT

SWIPE.csh spacewarp_2013-06-09.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 84664 new
mv new CFHTLS_production_retire_these.txt

# OK, these have to be repeats. Compare with yesterday's retiremnets.

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | grep uccess | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored" ; \
echo "List of subjects to be retired afresh:" ; \
wc -l new

# 58593 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#   26071 new

# OK, that's plausible! Off we go then:
mv new CFHTLS_production_retire_these.txt

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# All done!

# ======================================================================
# 2013-06-10 (Monday) 08:07 PDT

SWIPE.csh spacewarp_2013-06-10.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 64721 new
mv new CFHTLS_production_retire_these.txt

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | grep uccess | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored" ; \
echo "List of subjects to be retired afresh:" ; \
wc -l new

# 3058 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#    61663 new

mv new CFHTLS_production_retire_these.txt

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# 66072 retirement.log

# ======================================================================
# 2013-06-12 (Wednesday) 14:43 PDT

SWIPE.csh spacewarp_2013-06-12.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 10390 new
mv new CFHTLS_production_retire_these.txt

SWITCH.py CFHTLS_production_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-13 (Thursday) 08:35 PDT

# Today we compare simple and fuzzy trajectories! First do simple run,
# as control:

SWIPE.csh spacewarp_2013-06-13.gz
SWAPSHOP.csh -s CFHTLS --fast

sort -n CFHTLS_production_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 13908 new
mv new CFHTLS_production_retire_these.txt

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_production_retire_these.txt` )
   set k = `grep $subject retirement.log | grep uccess | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k > 1) then
       echo "Whoah! $subject was retired $k times yesterday!"
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored" ; \
echo "List of subjects to be retired afresh:" ; \
wc -l new

# 10390 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#     3518 new
mv new CFHTLS_production_retire_these.txt

# OK, don't retire these - let's now compare with the fuzzies!
# First need to accept Surhud's pull request, then check out the code
# into the Space Warps directory. If I decide to go back, I can just
# check out the previous version of SWAP... but Ill need a separate
# workspace, at the same level as "production". Call it: fuzzy.

# Need to get sartup.config right - check Surhud's emails.

# Here's the pull output:

# From github.com:drphilmarshall/SpaceWarps
#    d43ef4b..d77816a  master     -> origin/master
# Updating d43ef4b..d77816a
# Fast-forward
#  analysis/SWAP.py                |    6 ++
#  analysis/SWAPSHOP.csh           |    3 +
#  analysis/swap/agent.py          |   30 ++++++++
#  analysis/swap/bureau.py         |    8 +++
#  analysis/swap/collection.py     |   31 +++++---
#  analysis/swap/config.py         |    1 +
#  analysis/swap/logging.py        |   18 +++--
#  analysis/swap/production.config |    8 +--
#  analysis/swap/startup.config    |    6 +-
#  analysis/swap/subject.py        |   65 ++++++++++++-----
#  doc/sw-system.tex               |  151 ++++++++++++++++++---------------------
#  11 files changed, 204 insertions(+), 123 deletions(-)

# Forgot to tag it before pulling, oops. Ah well! Onwards :-)

SWAPSHOP.csh -s CFHTLS --fast --startup

# startup option brings over startup.config etc. Ntrajectory is set in
# the header of subject.py, which is a bug... But its set to 50
# as recommenedded by Surhud.

# Timing runs: each iteration of 50,000 classifications takes about a
# minute, a little over. With 5 million classifications, the whole
# SWAPSHOP run will take at least 100 mins - say 2 hours.

# Then, compare reports, and also lists of retirees. This needs to be
# done carefully, using grep and awk etc, not sdiff, probably.

# had to pause after 139 iterations - and it didn not restart cleanly
# :-(

SWAPSHOP.csh -s CFHTLS --fast

# OK, compare reports and retirements. Not easy, as we have to
# accumulate them uniquely. First clean up the fuzzy retirements!

sort -n CFHTLS_fuzzy_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 176818 new  GOOD!
mv new CFHTLS_fuzzy_retire_these.txt

mkdir -p comparison
cp CFHTLS_fuzzy_retire_these.txt CFHTLS_fuzzy_report.pdf comparison/

# OK, now bring in all retirements done so far from production...

cp ../production/CFHTLS_previously_retired.txt \
        comparison/CFHTLS_production_previously_retired.txt
cp ../production/CFHTLS_production_retire_these.txt comparison/

# Hmm - previous retirements will have doubles in them from repeated
# days efforts:

cd comparison

cat CFHTLS_production_previously_retired.txt | awk '{print $1}' | \
  grep 'ASW' | sort -n | uniq > new
wc -l CFHTLS_production_previously_retired.txt new
#   275170 CFHTLS_production_previously_retired.txt
#   177563 new
# OK, potentially very similar! Let's see.
mv new CFHTLS_production_previously_retired.txt

# Just to make sure, do same thing to the fuzzies:
cat CFHTLS_fuzzy_retire_these.txt | awk '{print $1}' | \
  grep 'ASW' | sort -n | uniq > new
wc -l CFHTLS_fuzzy_retire_these.txt new
#   176818 CFHTLS_fuzzy_retire_these.txt
#   176818 new
mv new CFHTLS_fuzzy_retire_these.txt

# Good. Now, we need two lists:
# 1) Subjects to retire now
# 2) Subjects to resurrect now

# 1) Take the fuzzy retirement list, and remove all that appear in
#    either the production previous retirements, or the production
#    retire these list.

cat CFHTLS_production_previously_retired.txt \
    CFHTLS_production_retire_these.txt | \
  awk '{print $1}' | grep 'ASW' | sort -n | uniq \
  > CFHTLS_production_all_retirements.txt

set retirements = CFHTLS_to_turn_fuzzy_retire_these.txt
set count = 0
\rm -f $retirements
foreach subject ( `cat CFHTLS_fuzzy_retire_these.txt` )
   set k = `grep $subject CFHTLS_production_all_retirements.txt | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> $retirements
   endif
end
echo "$count subjects already happily retired, can be ignored" ; \
echo "List of subjects to be retired:" ; \
wc -l $retirements

# 176463 subjects already happily retired, can be ignored
# List of subjects to be retired:
#      355 CFHTLS_to_turn_fuzzy_retire_these.txt

# Cool!


# 2) Take the production previously retired list, and remove the
#    subjects that appear in the fuzzy retirement list.

set resurrections = CFHTLS_to_turn_fuzzy_resurrect_these.txt
set count = 0
\rm -f $resurrections
foreach subject ( `cat CFHTLS_production_previously_retired.txt` )
   set k = `grep $subject CFHTLS_fuzzy_retire_these.txt | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> $resurrections
   endif
end
echo "$count subjects already happily retired, can be ignored" ; \
echo "List of subjects to be resurrected:" ; \
wc -l $resurrections

# 173060 subjects already happily retired, can be ignored
# List of subjects to be resurrected:
#     4503 CFHTLS_to_turn_fuzzy_resurrect_these.txt

# OK, execute this, then continue running SWITCH in fuzzy directory
# tomorrow? Yes - state is read from pickles, not from mongodb.

SWITCH.py CFHTLS_to_turn_fuzzy_retire_these.txt > turning_fuzzy_retirement.log &

SWITCH.py -r CFHTLS_to_turn_fuzzy_resurrect_these.txt > turning_fuzzy_resurrection.log &

# OK, done! Now, to start SWAPPING from directory fuzzy.

# ======================================================================
# 2013-06-15 (Saturday) 09:44 PDT

cd fuzzy

SWIPE.csh spacewarp_2013-06-15.gz
SWAPSHOP.csh -s CFHTLS --fast

# Had to restart after trashing production mongo directory and re-SWIPEing.

# SWAPSHOP: if you want, you can go ahead and retire 184711 subjects with
#
#           SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# Hmm. Seems like a lot! As if it didn't know what had been retired...
# Need to compare with log somehow.

sort -n CFHTLS_fuzzy_retire_these.txt | uniq | sed s/' '//g | grep 'ASW' > new
wc -l new
# 184711 new
mv new CFHTLS_fuzzy_retire_these.txt

# OK, try faking a retirement log!

cat comparison/CFHTLS_fuzzy_retire_these.txt > retirement.txt

# Now do the line by line comparison:

set count = 0
\rm -f new
foreach subject ( `cat CFHTLS_fuzzy_retire_these.txt` )
   set k = `grep $subject retirement.txt | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> new
   endif
end
echo "$count subjects scheduled for re-retirement, can be ignored" ; \
echo "List of subjects to be retired afresh:" ; \
wc -l new
# 176818 subjects scheduled for re-retirement, can be ignored
# List of subjects to be retired afresh:
#     7893 new

# Over-write with new file:
mv new CFHTLS_fuzzy_retire_these.txt

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# OK done - we are up and running.

# ======================================================================
# 2013-06-17 (Monday) 08:34 PDT

SWIPE.csh spacewarp_2013-06-17.gz

# Sort out these retirement lists. SWAPSHOP will concatenate
# previously_retired and the current copy of retire_these before
# over-writing - so we have to set up the former accordingly.

mv comparison/CFHTLS_fuzzy_retire_these.txt \
              CFHTLS_previously_retired.txt

SWAPSHOP.csh -s CFHTLS --fast

# SWAPSHOP: so far we have retired 184711 subjects. Let's do some more!
#   Good :-)
# ...
# SWAPSHOP: previous run brought total retirements to:
#   184711 CFHTLS_previously_retired.txt
# SWAPSHOP: current run has suggested another batch:
#   194387 CFHTLS_fuzzy_retire_these.txt
# SWAPSHOP: after filtering for repeats, the new retirements number:
#   194387 CFHTLS_fuzzy_retire_these.txt
# SWAPSHOP: after checking for uniqness, we need to retire these:
#     9676 CFHTLS_fuzzy_retire_these.txt
# SWAPSHOP: 184711 subjects were already retired and can be ignored

# OK, let's go!

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# Got interrupted, sigh.


# ======================================================================
# 2013-06-18 (Tuesday) 17:43 PDT

SWIPE.csh spacewarp_2013-06-18.gz

SWAPSHOP.csh -s CFHTLS --fast

# SWAPSHOP: 176772 subjects were already retired and can be ignored
# SWAPSHOP: if you want, you can go ahead and retire 4340 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-19 (Wednesday) 11:22 PDT

SWIPE.csh spacewarp_2013-06-19.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: after checking for uniqness, we need to retire these:
#     3239 CFHTLS_fuzzy_retire_these.txt
# SWAPSHOP: 0 subjects were already retired and can be ignored
# SWAPSHOP: if you want, you can go ahead and retire 3239 subjects

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-20 (Thursday) 08:57 PDT

SWIPE.csh spacewarp_2013-06-20.gz

SWAPSHOP.csh -s CFHTLS --fast

# SWAPSHOP: after checking for uniqness, we need to retire these:
#     3448 CFHTLS_fuzzy_retire_these.txt

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &
SWITCH.py CFHTLS_fuzzy_now_retire_these.txt >> retirement.log &

# ======================================================================
# 2013-06-21 (Friday) 05:52 PDT

SWIPE.csh spacewarp_2013-06-21.gz

SWAPSHOP.csh -s CFHTLS --fast

# SWAPSHOP: so far we have retired 205414 subjects. Let's do some more!
# SWAPSHOP: after checking for uniqness, we need to retire these:
#     2910 CFHTLS_fuzzy_retire_these.txt

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-22 (Saturday) 11:37 PDT

SWIPE.csh spacewarp_2013-06-22.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-23 (Sunday) 21:28 PDT

SWIPE.csh spacewarp_2013-06-23.gz
# SWIPE: mongorestore log stored in .spacewarp_2013-06-23_mongorestore.log
# SWIPE: ERROR: failed to restore database, exiting

# Huh?
more .spacewarp_2013-06-23_mongorestore.log
# Mon Jun 24 06:12:42.314 kern.sched unavailable
# couldn't connect to [127.0.0.1] couldn't connect to server 127.0.0.1:27017

# Try again from SLAC.

# ======================================================================
# 2013-06-24 (Monday) 09:46 PDT

SWIPE.csh spacewarp_2013-06-24.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-26 (Wednesday) 14:02 PDT

SWIPE.csh spacewarp_2013-06-26.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 6120 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-27 (Thursday) 14:59 PDT

SWIPE.csh spacewarp_2013-06-27.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2004 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-28 (Friday) 12:00 PDT

SWIPE.csh spacewarp_2013-06-28.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2788 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-29 (Saturday) 07:46 PDT

SWIPE.csh spacewarp_2013-06-29.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-06-30 (Sunday) 12:29 PDT

sleep 500

SWIPE.csh spacewarp_2013-06-30.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-01 (Monday) 14:10 PDT

SWIPE.csh spacewarp_2013-07-01.gz

SWAPSHOP.csh -s CFHTLS --fast
#SWAPSHOP: if you want, you can go ahead and retire 2419 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-02 (Tuesday) 06:31 PDT

SWIPE.csh spacewarp_2013-07-02.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2005 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-03 (Wednesday) 08:09 PDT

SWIPE.csh spacewarp_2013-07-03.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1803 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-03 (Wednesday) 08:09 PDT

SWIPE.csh spacewarp_2013-07-04.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-05 (Friday) 14:39 PDT

SWIPE.csh spacewarp_2013-07-05.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-06 (Saturday) 09:49 PDT

SWIPE.csh spacewarp_2013-07-06.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-07 (Sunday) 15:46 PDT

SWIPE.csh spacewarp_2013-07-07.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1270 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-08 (Monday) 06:23 PDT

SWIPE.csh spacewarp_2013-07-08.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-09 (Tuesday) 17:08 PDT

SWIPE.csh spacewarp_2013-07-09.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 658 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-11 (Thursday) 08:18 PDT

SWIPE.csh spacewarp_2013-07-11.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1023 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-12 (Friday) 08:52 PDT

SWIPE.csh spacewarp_2013-07-12.gz

SWAPSHOP.csh -s CFHTLS --fast
#SWAPSHOP: if you want, you can go ahead and retire 394 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &


# OK, we have time to try a full run, with plots. How much space is needed?

# 135 epochs, each with a 6Mb plot: 800Mb. Available disk space?
# 8.3Gb. Let's do it!

mkdirf fuzzy-with-plots
SWAPSHOP.csh -s CFHTLS --startup --animate

# Hmm - computer crashed, job stopped. Restart! Needs loaded Mongo, so wait
# until Sunday's production run is started!

SWAPSHOP.csh -s CFHTLS --animate

# ======================================================================
# 2013-07-14 (Sunday) 08:21 PDT

SWIPE.csh spacewarp_2013-07-14.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# Huh - no retirements! :-/

# ======================================================================
# 2013-07-15 (Monday) 10:58 PDT

SWIPE.csh spacewarp_2013-07-15.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 882 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# Hmm - maybe the other SWAP run interfered with things yesterday?

# ======================================================================
# 2013-07-16 (Tuesday) 08:35 PDT

SWIPE.csh spacewarp_2013-07-16.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-17 (Wednesday) 22:05 PDT

SWIPE.csh spacewarp_2013-07-17.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 130 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-18 (Thursday) 10:45 PDT

SWIPE.csh spacewarp_2013-07-18.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 168 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-19 (Friday) 10:17 PDT

SWIPE.csh spacewarp_2013-07-19.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 217 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-20 (Saturday) 21:28 PDT

SWIPE.csh spacewarp_2013-07-20.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 296 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-21 (Sunday) 08:24 PDT

SWIPE.csh spacewarp_2013-07-21.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-22 (Monday) 10:07 PDT

SWIPE.csh spacewarp_2013-07-22.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 554 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-24 (Wednesday) 08:43 PDT

SWIPE.csh spacewarp_2013-07-24.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2472 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-26 (Friday) 07:19 PDT

SWIPE.csh spacewarp_2013-07-26.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 3090 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-28 (Sunday) 10:29 PDT

SWIPE.csh spacewarp_2013-07-28.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 4330 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-29 (Monday) 12:32 PDT

SWIPE.csh spacewarp_2013-07-29.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1870 subjects with

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-07-30 (Tuesday) 08:29 PDT

SWIPE.csh spacewarp_2013-07-30.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_fuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-08-04 (Sunday) 17:27 PDT

SWIPE.csh spacewarp_2013-08-04.gz

SWAPSHOP.csh -s CFHTLS --fast

# SWAPSHOP: previous run brought total retirements to:
#   262756 CFHTLS_previously_retired.txt
# SWAPSHOP: current run has suggested another batch:
#   260243 CFHTLS_fuzzy_retire_these.txt
# SWAPSHOP: after checking for uniqness, we need to retire these:
#        0 CFHTLS_fuzzy_retire_these.txt
# SWAPSHOP: 260243 subjects were already retired and can be ignored
# SWAPSHOP: no subjects to retire

# Eh? Something odd happened here. Check retirement list:

wc -l CFHTLS_2013-0*/*retire_these.txt | tail
#   247869 CFHTLS_2013-07-20_10:10:07/CFHTLS_2013-07-20_10:10:07_retire_these.txt
#   248423 CFHTLS_2013-07-21_10:10:17/CFHTLS_2013-07-21_10:10:17_retire_these.txt
#   249244 CFHTLS_2013-07-22_10:12:05/CFHTLS_2013-07-22_10:12:05_retire_these.txt
#   251716 CFHTLS_2013-07-24_10:12:14/CFHTLS_2013-07-24_10:12:14_retire_these.txt
#   254806 CFHTLS_2013-07-26_10:08:30/CFHTLS_2013-07-26_10:08:30_retire_these.txt
#   259136 CFHTLS_2013-07-28_10:13:09/CFHTLS_2013-07-28_10:13:09_retire_these.txt
#   261006 CFHTLS_2013-07-29_10:14:18/CFHTLS_2013-07-29_10:14:18_retire_these.txt
#   262756 CFHTLS_2013-07-30_10:12:24/CFHTLS_2013-07-30_10:12:24_retire_these.txt
#     2513 CFHTLS_2013-08-04_10:26:49/CFHTLS_2013-08-04_10:26:49_retire_these.txt

# Why is August 4th's so short? OK, here's the problem:

# SWAPSHOP: so far we have retired 262756 subjects. Let's do some more!
# SWAPSHOP: starting batch number 1
# ================================================================================
#                    SWAP: the Space Warps Analysis Pipeline
# ================================================================================
# SWAP: updating all subjects with classifications made since 2013-07-30_10:12:24
# SWAP: read an old bureau of 31351 classification agents from ./CFHTLS_bureau.pickle
# SWAP: read an old collection of 296599 subjects from ./CFHTLS_collection.pickle
# SWAP: interpreting up to 50000  classifications...
# SWAP: .........................................................................
# SWAP: total no. of classifications processed:  50000
# SWAP: saving agents to ./CFHTLS_bureau.pickle
# SWAP: saving subjects to ./CFHTLS_collection.pickle
# ERROR: IOError: [Errno 28] No space left on device [swap.io]

# Damn - so new pickle was started in the next batch :-(

# Only solution is to re-run from scratch! And then manually compare the
# retirement list generated with the one currently marked as "previously
# retired". Do this in a new directory: newfuzzy

# ======================================================================
# 2013-08-05 (Monday) 09:13 PDT

# Note to self: when re-activating at Stage 2, do not do ASW000000001 through
# f - these have problematic labels, and in any case reappear in D11 without
# sims in them.

# ======================================================================
# 2013-08-05 (Monday) 09:15 PDT

# Restarting from scratch! Do it fast though, no need for more plots.
# Also, implement hasty=False in config, to run using all classifications,
# even on subjects that should have been retired. This way we will use
# all classifications! :-) Report should show all classifications used.

# BTW, need to kill old mongo dir.

mkdirf $SWAP_DIR/allfuzzy
# /Users/pjm/public_html/SpaceWarps/Science/analysis/allfuzzy

SWIPE.csh spacewarp_2013-08-05.gz

# Copy old fuzzy retirees to here, so that the comparison can be done by
# SWAPSHOP:

cp ../fuzzy/CFHTLS_previously_retired.txt .
cp ../fuzzy/CFHTLS_fuzzy_retire_these.txt .

# (The second file is empty.)

# Now the analysis:

SWAPSHOP.csh -s CFHTLS --fast --startup

# SWAPSHOP: start-up configuration stored in startup.config
# SWAPSHOP: so far we have retired 262756 subjects. Let's do some more!
# ...
# SWAPSHOP: previous run brought total retirements to:
#   262756 CFHTLS_previously_retired.txt
# SWAPSHOP: current run has suggested another batch:
#    18003 CFHTLS_allfuzzy_retire_these.txt
# SWAPSHOP: after checking for uniqness, we need to retire these:
#     9352 CFHTLS_allfuzzy_retire_these.txt
# SWAPSHOP: 8651 subjects were already retired and can be ignored
# SWAPSHOP: if you want, you can go ahead and retire 9352 subjects with


# Now compare this retirement list with the old fuzzy one:

# wc -l CFHTLS_*/*retire*
#   263457 CFHTLS_2013-08-05_10:14:39/CFHTLS_2013-08-05_10:14:39_retire_these.txt

# Retirements required are in
wc -l CFHTLS_allfuzzy_retire_these.txt
#    9352 CFHTLS_allfuzzy_retire_these.txt

# Are there any resurrections needed? These are the subjects that are in the
# old fuzzy retirement list, but not in the new one.

wc -l ../fuzzy/CFHTLS_previously_retired.txt
#   262756 ../fuzzy/CFHTLS_previously_retired.txt
cp ../fuzzy/CFHTLS_previously_retired.txt CFHTLS_fuzzy_previously_retired.txt

# Difference is echo "263457 - 262756" | bc = 701

# Why is this different from the new number of retirements?
#  -> Some of the previously retired systems need resurrecting.

# We might expect to have to resurrect
# (9352-701 = 8651) systems. Need to make this resurrection list.

# From above:
#  Take the previously retired list, and remove the
#  subjects that appear in the fuzzy retirement list.
# The leftovers are to be resurrected!

set resurrections = CFHTLS_allfuzzy_resurrect_these.txt
set allfuzzyretirements = CFHTLS_2013-08-05_10:14:39/CFHTLS_2013-08-05_10:14:39_retire_these.txt
set fuzzyretirements = CFHTLS_fuzzy_previously_retired.txt
set newretirements = CFHTLS_allfuzzy_retire_these.txt

set count = 0
\rm -f $resurrections
foreach subject ( `cat $fuzzyretirements` )
   set k = `grep $subject $allfuzzyretirements | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> $resurrections
   endif
end
echo "$count subjects already happily retired, can be ignored" ; \
echo "List of subjects to be resurrected:" ; \
wc -l $resurrections

# 253852 subjects already happily retired, can be ignored
# List of subjects to be resurrected:
#     8904 CFHTLS_allfuzzy_resurrect_these.txt

# Some mismatch: this is not the expected 8651.
#  - could some resurrections be in NEW retirement list?

set count = 0
foreach subject ( `cat $newretirements` )
   set k = `grep $subject $resurrections | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   endif
end
echo "$count new retirements will have just been resurrected!"

# 0 new retirements will have just been resurrected!
# Hmm.

# Oh well. Do 8904 resurrections, then 9532 retirements, then
# start afresh tomorrow using the allfuzzy pickles.

SWITCH.py -r CFHTLS_allfuzzy_resurrect_these.txt >& CFHTLS_allfuzzy_resurrect_these.log

# List includes 14 at the beginning that are not to be resurrected - but
# script takes care of them.

SWITCH.py CFHTLS_allfuzzy_retire_these.txt >& CFHTLS_allfuzzy_retire_these.log

# OK, now need to make the correct previously retired list, for SWAPSHOP:

cp $allfuzzyretirements CFHTLS_allfuzzy_previously_retired.txt

# OK, SWAPSHOP will concatenate this with CFHTLS_allfuzzy_retire_these.txt
# the next time the script runs.



# Check stats on report!

cp CFHTLS_2013-08-05_10:14:39/CFHTLS_2013-08-05_10:14:39_report.pdf \
   CFHTLS_allfuzzy_2013-08-05_10:14:39_report.pdf

# False negative rate is down to 2.9%
# Lens completeness is 93.5%
# Lens purity is 2.1% (1708 candidates)
# Mean classifications per subject: 23.3

# Compare with most recent meaningful fuzzy report:

cp ../fuzzy/CFHTLS_2013-07-30_10:12:24/CFHTLS_2013-07-30_10:12:24_report.pdf \
   CFHTLS_fuzzy_2013-07-30_10:12:24_report.pdf

# FN: 7.1%
# C:  92.2%
# P:  0.1% (2153 candidates)
# Nc: 8.0

# GREAT. However, these numbers depend on the sims, which are classified
# hundreds of times each: so they may well not be accurate :-/ Not sure what
# to do about this. Previous system (hasty: True) had systems ignored,
# and then retired, as soon as they crossed threshold - the same wa strue for
# the sims. Return to this mode later?

# ======================================================================
# 2013-07-30 (Tuesday) 08:29 PDT

SWIPE.csh spacewarp_2013-08-07.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: previous run brought total retirements to:
#   263457 CFHTLS_allfuzzy_previously_retired.txt
# SWAPSHOP: current run has suggested another batch:
#     4411 CFHTLS_allfuzzy_retire_these.txt
# SWAPSHOP: after checking for uniqness, we need to retire these:
#     3853 CFHTLS_allfuzzy_retire_these.txt
# SWAPSHOP: 558 subjects were already retired and can be ignored
# SWAPSHOP: if you want, you can go ahead and retire 3853 subjects with

SWITCH.py CFHTLS_allfuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-08-08 (Thursday) 13:36 PDT

SWIPE.csh spacewarp_2013-08-08.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: 665 subjects were already retired and can be ignored
# SWAPSHOP: if you want, you can go ahead and retire 3416 subjects with

SWITCH.py CFHTLS_allfuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-08-09 (Friday) 14:34 PDT

SWIPE.csh spacewarp_2013-08-09.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 3118 subjects with

SWITCH.py CFHTLS_allfuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-08-12 (Monday) 15:52 PDT

SWIPE.csh spacewarp_2013-08-12.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 5864 subjects with

SWITCH.py CFHTLS_allfuzzy_retire_these.txt > retirement.log &

# Also, start fuzzy processing again, for comparison. Possible to run both?
# Want to switch back to the hasty processing, in order to keep
# selection function clean. Run fuzzy from the beginning! Probably should do
# this on Weds, when we have lots of time... Start early at SLAC, then
# leave while at football. Pick up laptop on the way home.

# ======================================================================
# 2013-08-13 (Tuesday) 13:44 PDT

SWIPE.csh spacewarp_2013-08-13.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1540 subjects with

SWITCH.py CFHTLS_allfuzzy_retire_these.txt > retirement.log &

# ======================================================================
# 2013-08-14 (Wednesday) 10:36 PDT

# OK, switching back to hasty analysis, in analysis/hasty !

# First kill off old mongo, in allfuzzy:
\rm -rf spacewarp_2013-08-13* mongo

# Then:

cd ../hasty

SWIPE.csh spacewarp_2013-08-14.gz

# Edit startup.config to make hasty standard. Then run swapshop:

SWAPSHOP.csh -s CFHTLS --fast --startup

# OK, started at 12:27 PDT, finished 12.5 hours later!

# SWAPSHOP: previous run brought total retirements to:
#        0 CFHTLS_hasty_previously_retired.txt
# SWAPSHOP: current run has suggested another batch:
#   283791 CFHTLS_hasty_retire_these.txt
# SWAPSHOP: after checking for uniqness, we need to retire these:
#   283791 CFHTLS_hasty_retire_these.txt
# SWAPSHOP: 0 subjects were already retired and can be ignored
# SWAPSHOP: if you want, you can go ahead and retire 283791 subjects with

# Stats from the report are:
#
# Number of classifications: 	7924951
# Number of classns used: 		2845249
# Number of classifiers: 		31808
# Number of test subjects: 		295030
# Number of sims: 				3895
# Number of duds: 				3150
#
# Mean test classns/classifier:	223.8
# Mean classns/test subject:	9.2
# Test subject retirements:		283791
# Mean classns/retirement:		9.1
# Test subject rejections:		283791
# Test subject identifications:	2315
# Lens completeness:			91.7%
# Lens purity:					0.2%
# FP contamination:				99.8%
# Lenses missed (FN rate):		7.9%

# OK, now need to compare hasty retirements with most recent allfuzzy list,
# as above.

# From above:
#  Take the previously retired list, and remove the
#  subjects that appear in the fuzzy retirement list.
# The leftovers are to be resurrected!

set resurrections = CFHTLS_hasty_resurrect_these.txt
set allfuzzyretirements = ../allfuzzy/CFHTLS_2013-08-13_10:01:43/CFHTLS_2013-08-13_10:01:43_retire_these.txt
set hastyretirements = ../hasty/CFHTLS_2013-08-14_10:03:28/CFHTLS_2013-08-14_10:03:28_retire_these.txt
set newretirements = CFHTLS_hasty_actually_retire_these.txt

set count = 0
\rm -f $resurrections
foreach subject ( `cat $allfuzzyretirements` )
   set k = `grep $subject $hastyretirements | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> $resurrections
   endif
end
echo "$count allfuzzy subjects already happily retired, can be ignored" ; \
echo "List of subjects to be resurrected:" ; \
wc -l $resurrections

# 279390 allfuzzy subjects already happily retired, can be ignored
# List of subjects to be resurrected:
#      840 CFHTLS_hasty_resurrect_these.txt


# Now look for new retirements, by doing the cross-check the other way:

set count = 0
\rm -f $newretirements
foreach subject ( `cat $hastyretirements` )
   set k = `grep $subject $allfuzzyretirements | wc -l`
   if ($k == 1) then
       @ count = $count + $k
   else if ($k == 0) then
       echo $subject >> $newretirements
   endif
end
echo "$count hasty subjects already happily retired, can be ignored" ; \
echo "List of subjects to be retired:" ; \
wc -l $newretirements

# 279390 hasty subjects already happily retired, can be ignored
# List of subjects to be retired:
#     4401 CFHTLS_hasty_actually_retire_these.txt

# Next time around, the $hastyretirements list will get copied into the
# previously retired list, so no need to copy manually.

# Now SWITCH!

SWITCH.py -r CFHTLS_hasty_resurrect_these.txt >& CFHTLS_hasty_resurrect_these.log

# Done.

SWITCH.py CFHTLS_hasty_actually_retire_these.txt >& CFHTLS_hasty_actually_retire_these.log

# Pending...

# Meanwhile, in the allfuzzy directory, run SWAPSHOP to keep the pickles
# up to date.

cd $SWAP_DIR/allfuzzy
SWAPSHOP.csh -s CFHTLS --fast

# Save transfer files:

cd $SWAP_DIR/hasty

mkdir transfer
mv CFHTLS_hasty_actually_retire_these.* transfer/
mv CFHTLS_hasty_resurrect_these.* transfer/
cp CFHTLS_hasty_* transfer/


# ======================================================================
# 2013-08-16 (Friday) 09:09 PDT

# New regime: both hasty and allfuzzy!

SWIPE.csh spacewarp_2013-08-16.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2489 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log &

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-17 (Saturday) 15:25 PDT

SWIPE.csh spacewarp_2013-08-17.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 863 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log &

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-18 (Sunday) 09:41 PDT

SWIPE.csh spacewarp_2013-08-18.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 715 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-19 (Monday) 10:34 PDT

SWIPE.csh spacewarp_2013-08-19.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 502 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-20 (Tuesday) 09:32 PDT

SWIPE.csh spacewarp_2013-08-20.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 457 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-21 (Wednesday) 13:08 PDT

SWIPE.csh spacewarp_2013-08-21.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 352 subjects with
# Low because D8 is now in play...

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-22 (Thursday) 10:48 PDT

SWIPE.csh spacewarp_2013-08-22.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 424 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-23 (Friday) 08:56 PDT

SWIPE.csh spacewarp_2013-08-23.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 766 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-24 (Saturday) 10:13 PDT

SWIPE.csh spacewarp_2013-08-24.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAP: updating all subjects with classifications made since 2013-08-23_10:14:02
# SWAP: should we use the marker positions on sims?  True
# SWAP: read an old bureau of 32175 classification agents from ./CFHTLS_bureau.pickle
# SWAP: read an old collection of 326666 subjects from ./CFHTLS_collection.pickle
# SWAP: interpreting up to 50000  classifications...
#
# SWAP: total no. of classifications processed:  0
# SWAP: something went wrong - 0 classifications found.

# SWIPE: mongorestoring into database 'ouroboros'
# SWIPE: mongorestore log stored in .spacewarp_2013-08-24_mongorestore.log
# SWIPE: ERROR: failed to restore database, exiting

# Hmm - not sure why. Trash 24, make diskspace, try 25.

# ======================================================================
# 2013-08-25 (Sunday) 15:33 PDT

SWIPE.csh spacewarp_2013-08-25.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1547 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-26 (Monday) 17:20 PDT

SWIPE.csh spacewarp_2013-08-26.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1369 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-27 (Tuesday) 08:47 PDT

SWIPE.csh spacewarp_2013-08-27.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1925 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-28 (Wednesday) 17:24 PDT

SWIPE.csh spacewarp_2013-08-28.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2398 subjects with
# That's better!

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-29 (Thursday) 10:29 PDT

SWIPE.csh spacewarp_2013-08-29.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2089 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-08-30 (Friday) 14:29 PDT

SWIPE.csh spacewarp_2013-08-30.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1675 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-09-01 (Sunday) 18:38 PDT

SWIPE.csh spacewarp_2013-09-01.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 4024 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-09-02 (Monday) 11:09 PDT

SWIPE.csh spacewarp_2013-09-02.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1935 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# ======================================================================
# 2013-09-03 (Tuesday) 11:56 PDT

SWIPE.csh spacewarp_2013-09-03.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2303 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Now update allfuzzy pickles:

cd $SWAP_DIR/allfuzzy

SWAPSHOP.csh -s CFHTLS --fast

cd $SWAP_DIR/hasty

# Damn - allfuzzy pickles are corrupted :-/ Will need to be restarted.

# ======================================================================
# 2013-09-04 (Wednesday) 11:09 PDT

# Hasty-only analysis.

SWIPE.csh spacewarp_2013-09-04.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2003 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-06 (Friday) 12:13 PDT

SWIPE.csh spacewarp_2013-09-06.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# Oops, this SWIPE failed.

# ======================================================================
# 2013-09-07 (Saturday) 13:33 PDT

SWIPE.csh spacewarp_2013-09-07.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 7896 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-09 (Monday) 12:17 PDT

SWIPE.csh spacewarp_2013-09-09.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 5449 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-10 (Tuesday) 09:24 PDT

SWIPE.csh spacewarp_2013-09-10.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1087 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-11 (Wednesday) 09:58 PDT

SWIPE.csh spacewarp_2013-09-11.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1094 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-12 (Thursday) 12:30 PDT

SWIPE.csh spacewarp_2013-09-12.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1213 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-13 (Friday) 10:14 PDT

SWIPE.csh spacewarp_2013-09-13.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1737 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-15 (Sunday) 18:47 PDT

SWIPE.csh spacewarp_2013-09-15.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 4421 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-16 (Monday) 08:38 PDT

SWIPE.csh spacewarp_2013-09-16.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 3402 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-17 (Tuesday) 09:33 PDT

SWIPE.csh spacewarp_2013-09-17.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2234 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-18 (Wednesday) 12:28 PDT

SWIPE.csh spacewarp_2013-09-18.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2539 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-19 (Thursday) 10:56 PDT

SWIPE.csh spacewarp_2013-09-19.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1965 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-20 (Friday) 09:41 PDT

sleep 600

SWIPE.csh spacewarp_2013-09-20.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2136 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-23 (Monday) 08:33 PDT

SWIPE.csh spacewarp_2013-09-23.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 5722 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-24 (Tuesday) 09:39 PDT

SWIPE.csh spacewarp_2013-09-24.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1921 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-25 (Wednesday) 11:09 PDT

SWIPE.csh spacewarp_2013-09-25.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2039 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-26 (Thursday) 12:23 PDT

SWIPE.csh spacewarp_2013-09-26.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1922 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-27 (Friday) 10:10 PDT

SWIPE.csh spacewarp_2013-09-27.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1863 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-09-29 (Sunday) 17:31 PDT

SWIPE.csh spacewarp_2013-09-29.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 4207 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-01 (Tuesday) 10:34 PDT

SWIPE.csh spacewarp_2013-10-01.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 4739 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-02 (Wednesday) 08:09 PDT

SWIPE.csh spacewarp_2013-10-02.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1478 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-03 (Thursday) 17:58 PDT

SWIPE.csh spacewarp_2013-10-03.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-04 (Friday) 16:04 PDT

SWIPE.csh spacewarp_2013-10-04.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1859 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-07 (Monday) 01:08 PDT

SWIPE.csh spacewarp_2013-10-06.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 3805 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-07 (Monday) 08:37 PDT

SWIPE.csh spacewarp_2013-10-07.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1741 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-08 (Tuesday) 14:13 CEST

SWIPE.csh spacewarp_2013-10-08.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1361 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-09 (Wednesday) 14:12 PDT

SWIPE.csh spacewarp_2013-10-09.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1691 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-10 (Thursday) 11:33 PDT

SWIPE.csh spacewarp_2013-10-10.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1550 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-11 (Friday) 09:09 PDT

SWIPE.csh spacewarp_2013-10-11.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-14 (Monday) 08:47 PDT

SWIPE.csh spacewarp_2013-10-14.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 4186 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-16 (Wednesday) 11:20 PDT

SWIPE.csh spacewarp_2013-10-16.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-17 (Thursday) 12:33 PDT

SWIPE.csh spacewarp_2013-10-17.gz

# Argh - pickles corrupted. Redo all hasty analysis.

SWAPSHOP.csh -s CFHTLS --fast --startup
# SWAPSHOP: if you want, you can go ahead and retire 392430 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-19 (Saturday) 10:12 PDT

SWIPE.csh spacewarp_2013-10-18.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1069 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-21 (Monday) 11:23 PDT

SWIPE.csh spacewarp_2013-10-21.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 4499 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-22 (Tuesday) 14:27 PDT

SWIPE.csh spacewarp_2013-10-22.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2013 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-23 (Wednesday) 15:49 PDT

SWIPE.csh spacewarp_2013-10-23.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1025 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-24 (Thursday) 13:29 PDT

SWIPE.csh spacewarp_2013-10-24.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1719 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-25 (Friday) 11:59 PDT

SWIPE.csh spacewarp_2013-10-25.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1491 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-28 (Monday) 11:24 PDT

SWIPE.csh spacewarp_2013-10-28.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 4245 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-29 (Tuesday) 13:49 CDT

SWIPE.csh spacewarp_2013-10-29.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1488 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-10-30 (Wednesday) 11:03 CDT

SWIPE.csh spacewarp_2013-10-30.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 933 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-02 (Saturday) 09:51 PDT

SWIPE.csh spacewarp_2013-11-01.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 2402 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-04 (Monday) 09:31 PST

SWIPE.csh spacewarp_2013-11-04.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 3073 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-05 (Tuesday) 09:38 PST

SWIPE.csh spacewarp_2013-11-05.gz
# x spacewarp_2013-11-05/spacewarp_classifications.bson: Truncated tar archive
# tar: Error exit delayed from previous errors.

# Hmm - see if tomorrow's is better...

# ======================================================================
# 2013-11-06 (Wednesday) 10:07 PST

SWIPE.csh spacewarp_2013-11-06.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 1511 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-07 (Thursday) 08:32 PST

SWIPE.csh spacewarp_2013-11-07.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 680 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-08 (Friday) 12:47 PST

SWIPE.csh spacewarp_2013-11-08.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 442 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-09 (Saturday) 13:24 PST

SWIPE.csh spacewarp_2013-11-09.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 414 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-11 (Monday) 09:35 PST

SWIPE.csh spacewarp_2013-11-11.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAP: something went wrong - 0 classifications found.

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-14 (Thursday) 09:52 PST

SWIPE.csh spacewarp_2013-11-14.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 784 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-15 (Friday) 14:13 PST

SWIPE.csh spacewarp_2013-11-15.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-18 (Monday) 11:02 PST

SWIPE.csh spacewarp_2013-11-18.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-19 (Tuesday) 12:06 PST

SWIPE.csh spacewarp_2013-11-19.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-20 (Wednesday) 14:52 PST

SWIPE.csh spacewarp_2013-11-20.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 30 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-21 (Thursday) 10:04 PST

SWIPE.csh spacewarp_2013-11-21.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 46 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-22 (Friday) 11:45 PST

sleep 500
SWIPE.csh spacewarp_2013-11-22.gz && \
SWAPSHOP.csh -s CFHTLS --fast && \
SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# SWAPSHOP: if you want, you can go ahead and retire 20 subjects with

# There's something funny going on here - 13,000 classifications and
# only 20 retirements?

# Probably we are down to the detections and the undecideds! Finishing
# criterion may need resetting - via max classifications = 20?

# ======================================================================
# 2013-11-25 (Monday) 08:35 PST

SWIPE.csh spacewarp_2013-11-25.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 77 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-26 (Tuesday) 12:11 PST

SWIPE.csh spacewarp_2013-11-26.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 14 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-27 (Wednesday) 12:59 PST

SWIPE.csh spacewarp_2013-11-27.gz

SWAPSHOP.csh -s CFHTLS --fast
# SWAPSHOP: if you want, you can go ahead and retire 31 subjects with

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# ======================================================================
# 2013-11-27 (Wednesday) 15:02 PST

# Investigating undecideds. Hampered by SWAP's enormous (1.8Gb!) pickle,
# so try developing in a testplots directory having run SWAP on just the
# autumn data (since Nov 1).

mkdirf testplots
cp ../hasty/startup.config autumn.config

# Edited startup.config...

SWAP.py autumn.config

# Takes some time to get to the right part of teh file... Then does 50k,
# and stops. Good! Now have pickles for plot testing.

SWAG.py update.config

# This worked well for testing. Then I upgraded SWAP to output catalogs with
# probabilities, and an extra trajectories plot showing *all* the
# undecided/detected test subjects, sims and duds.
# I ran SWAP on a copy of update.config, that had report=True and
# repickle=False:

SWAP.py plot.config

# ...
# SWAP: updating all subjects with classifications made since 2013-11-27_10:20:43
# SWAP: read an old bureau of 36536 classification agents from ./CFHTLS_bureau.pickle
# SWAP: read an old collection of 437276 subjects from ./CFHTLS_collection.pickle
# ...
# SWAP: saving lens candidates...
# SWAP: 3348 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/hasty/CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_candidates.txt
# ...
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 6409 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/hasty/CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 5337 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/hasty/CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 5 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/hasty/CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_dud_catalog.txt
# ...
# SWAP: plotting 500 candidates in /Users/pjm/public_html/SpaceWarps/Science/analysis/hasty/CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_sample.png
# SWAP: plotting 500 sims in /Users/pjm/public_html/SpaceWarps/Science/analysis/hasty/CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_sample.png
# SWAP: plotting 5 duds in /Users/pjm/public_html/SpaceWarps/Science/analysis/hasty/CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_sample.png

# So, 3348 candidates, (6409-3348) = 3061 undecideds.

# OK, look at completeness as a function of probability threshold.
# 5712 sims in total: 5337 sim candidates = 93.4% completeness

set candidatecat = CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_candidate_catalog.txt
set simcat = CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_sim_catalog.txt

set thresholds = ( 0.01 0.05 0.10 0.25 0.50 0.90 0.95 )

foreach threshold ( $thresholds )
   set Ncandidates = `grep -v '#' $candidatecat | awk '{if ($2 > '$threshold') print $1}' | wc -l`
   set Nsims = `grep -v '#' $simcat | awk '{if ($2 > '$threshold') print $1}' | wc -l`
   set completeness = `echo $Nsims 5712 | awk '{printf "%.1f\n", 100.0*$1/$2}'`
   echo "P > ${threshold}: ${Ncandidates} candidates at ${completeness}% completeness"
end

# P > 0.01: 3837 candidates at 93.0% completeness
# P > 0.05: 3675 candidates at 92.9% completeness
# P > 0.10: 3609 candidates at 92.9% completeness
# P > 0.25: 3514 candidates at 92.9% completeness
# P > 0.50: 3456 candidates at 92.8% completeness
# P > 0.90: 3358 candidates at 92.8% completeness
# P > 0.95: 3348 candidates at 92.8% completeness

# Interesting. I think this shows that accepting candidates below
# P = 0.95 is barely worth it - we're adding a lot of noise for very little
# gain in completeness. Distribution of P's is highly bimodal!

# How many subjects have P > rejection, P < detection and Nclass < 20?

grep -v '#' $candidatecat | awk '{if ($2 < 0.95 && $3 < 20) print $1}' | wc -l
#     1403
grep -v '#' $candidatecat | awk '{if ($2 < 0.95 && $3 < 10) print $1}' | wc -l
#     1062

# This is how many "unfinished" subjects there are. If we set
# max_classifications to 20, we only have 1403 images to go. Request from
# Michael!

# OK, what about even better samples? Can we select at P > 99%?

set thresholds = ( 0.95 0.96 0.97 0.99 0.995 0.999 )

foreach threshold ( $thresholds )
   set Ncandidates = `grep -v '#' $candidatecat | awk '{if ($2 > '$threshold') print $1}' | wc -l`
   set Nsims = `grep -v '#' $simcat | awk '{if ($2 > '$threshold') print $1}' | wc -l`
   set completeness = `echo $Nsims 5712 | awk '{printf "%.1f\n", 100.0*$1/$2}'`
   echo "P > ${threshold}: ${Ncandidates} candidates at ${completeness}% completeness"
end

# P > 0.95: 3348 candidates at 92.8% completeness
# P > 0.96: 2921 candidates at 82.9% completeness
# P > 0.97: 2463 candidates at 71.8% completeness
# P > 0.99: 1153 candidates at 37.6% completeness
# P > 0.995: 581 candidates at 23.8% completeness
# P > 0.999: 37 candidates at 2.2% completeness

# Interesting: 0.95 looks somewhat special, the completeness drops off above
# that number. What do the 0.999 systems look like? ~ the top 40?

mkdir -p top40
set urls = `grep -v '#' $candidatecat | awk '{if ($2 > 0.999) print $4}'`
foreach url ( $urls )
  set x = `grep $url $candidatecat`
  set png = "top40/${x[1]}.png"
  wget -O $png -o junk $url
  du -h $png
end
cd top40
gallery.pl -o top40.pdf -x 2 -y 2 -pdf -t *png

# Hah! Only a few look like solid lenses. Stage 1 P is noisy...

# ======================================================================
# 2013-12-02 (Monday) 15:33 PST

# Look up probabilities of real lenses!
# Do this in hasty directory.

set manifest = /Users/pjm/public_html/SpaceWarps/Science/training/SpottersGuide/CFHTLS/knownlenses/listfiles_d1_d11
set lenscat = /Users/pjm/public_html/SpaceWarps/Science/training/SpottersGuide/CFHTLS/knownlenses/finlenscat_updated.txt
set candidatecat = CFHTLS_2013-11-27_10:20:43/CFHTLS_2013-11-27_10:20:43_candidate_catalog.txt

\rm -f junk*
foreach stem ( `grep -v '#' $lenscat | awk '{print $1}'` )
  set zooid = `grep $stem $manifest | awk '{print $3}'`
  foreach k ( 2 3 )
    set altstem = `echo $stem | sed s/CFHTLS_0/CFHTLS_$k/g | sed s/CFHTLS_1/CFHTLS_$k/g`
    set altzooid = `grep $altstem $manifest | awk '{print $3}'`
    if ($#altzooid) then
      set stem = $altstem
      set zooid = $altzooid
    endif
  end
  grep $zooid $candidatecat >> junk1
  grep $stem $lenscat >> junk2
  grep $zooid $manifest >> junk3
end

# Note how we had to check for teh alt ids, for teh subjects that have real
# lenses in them AND had sims put in them. The later ids are from D11, when
# we put these subjects back in without sims.

paste junk1 junk2 junk3 > ${candidatecat:r}_knownlenses.txt
# This is wrong - the components don't match up!

wc -l junk*
     133 junk1
     199 junk2
     216 junk3

# How many real lenses are there?
awk '{print $1}' junk2 | sort -n | uniq | wc -l
# 163
# So not 216!...

# And how many are candidates?
awk '{print $1}' junk1 | sort -n | uniq | wc -l
#       121

# So, 121 out of 163 - 74%.

# Grab 2 columns, uniq by 1 and sort by 2?

awk '{print $1,$2}' junk1 | sort -n | uniq | sort -rn -k2 | tail -20
# ASW0004p94 0.9515456
# ASW00099md 0.9510602
# ASW0009cro 0.5296275
# ASW0005aj2 0.0881583
# ASW0009clg 0.0525023
# ASW0000dlz 0.0206259
# ASW0009bn8 0.0059907
# ASW00056pq 0.0011886
# ASW000037r 0.0004669
# ASW00099dy 0.0004428
# ASW0009d7g 0.0000261
# ASW0008mw7 0.0000027

# OK: all but 10 have P > 0.95!
# So 111 make candidates list, out of 163: 68% complete.
# Full catalog including undecideds is 121/163 = 74% complete.

# BUG: some "lenses" in the lenscat are actually training images (they
# had sims put in them!) - so they do not appear in the candidate catalog,
# just because they are not test subjects. Caught and reran, code above is
# now correct.

# ======================================================================
# 2013-12-04 (Wednesday) 07:31 EST

# Make galleries of real lenses that are detected, undecided and rejected

mkdir -p detected
cd detected
set detected = `awk '{if ($2 > 0.95) print $1}' ../junk1 | sort -n | uniq`
set pngs = ()
foreach subject ( $detected )
  set url = `grep $subject ../junk1 | head -1 | awk '{print $4}'`
  set zooid = `grep $subject ../junk1 | head -1 | awk '{print $1}'`
  wget -O ${zooid}.png "$url"
  set pngs = ( $pngs ${zooid}.png )
end
gallery.pl -pdf -x 2 -y 2 -t -o gallery_detected.pdf $pngs
cd ../

# set new = `ls $pngs | & grep directory | cut -d':' -f2 | cut -d'.' -f1`
# foreach subject ( $new )
#   set url = `grep $subject ../junk1 | head -1 | awk '{print $4}'`
#   set zooid = `grep $subject ../junk1 | head -1 | awk '{print $1}'`
#   wget -O ${zooid}.png "$url"
# end


mkdir -p undecided
cd undecided
set undecided = `awk '{if ($2 < 0.95) print $1}' ../junk1 | sort -n | uniq`
set pngs = ()
foreach subject ( $undecided )
  set url = `grep $subject ../junk1 | head -1 | awk '{print $4}'`
  set zooid = `grep $subject ../junk1 | head -1 | awk '{print $1}'`
  wget -O ${zooid}.png "$url"
  set pngs = ( $pngs ${zooid}.png )
end
gallery.pl -pdf -x 2 -y 2 -t -o gallery_undecided.pdf $pngs
cd ../

# set new = `ls $pngs | & grep directory | cut -d':' -f2 | cut -d'.' -f1`
# foreach subject ( $new )
#   set url = `grep $subject ../junk1 | head -1 | awk '{print $4}'`
#   set zooid = `grep $subject ../junk1 | head -1 | awk '{print $1}'`
#   wget -O ${zooid}.png "$url"
# end


mkdir -p rejected
cd rejected
set lenses = ( $detected $undecided )
set all = `awk '{print $3}' ../junk3 | sort -n | uniq`
set pngs = ()
foreach subject ( $all )
  set k = 0
  set ignore = 0
  while ($k < $#lenses)
    @ k = $k + 1
    if ($subject == $lenses[$k]) then
      set ignore = 1
      set k = 100000
    endif
  end
  if ($ignore == 0) then
    set url = `grep $subject ../junk3 | head -1 | awk '{print $1}'`
    set zooid = `grep $subject ../junk3 | head -1 | awk '{print $3}'`
    # wget -N -O ${zooid}.png "$url"
    set pngs = ( $pngs ${zooid}.png )
  endif
end
gallery.pl -pdf -x 2 -y 2 -t -o gallery_rejected.pdf $pngs
cd ../

# set new = `ls $pngs | & grep directory | cut -d':' -f2 | cut -d'.' -f1`
# foreach subject ( $new )
#   set url = `grep $subject ../junk3 | head -1 | awk '{print $1}'`
#   set zooid = `grep $subject ../junk3 | head -1 | awk '{print $3}'`
#   wget -O ${zooid}.png "$url"
# end


# ======================================================================
# 2013-12-08 (Sunday) 15:32 GMT

# In Oxfordshire, haven't run for a while!

SWIPE.csh spacewarp_2013-12-08.gz

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# SWAP: interpreting up to 50000  classifications...
# SWAP:
# SWAP: total no. of classifications processed:  1

# Something wrong...
# Mongo was the old one, from doing the testplots. Killed it, can now
# move on!

SWAPSHOP.csh -s CFHTLS --fast

SWITCH.py CFHTLS_hasty_retire_these.txt > retirement.log

# OK good! 100 more retirements.

# Now have 3381 candidates - send these to Adler for Stage 2!
# Final analysis time is CFHTLS_2013-12-08_10:20:56

# Cut catalog at 0.95:

set cat = CFHTLS_2013-12-08_10:20:56/CFHTLS_2013-12-08_10:20:56_candidate_catalog.txt

set finalcat = CFHTLS_2013-12-08_10:20:56_good_candidate_catalog.txt

head -1 $cat > $finalcat
grep -v '#' $cat | awk '{if ($2 > 0.95) print $0}' >> $finalcat

# Make a copy in Science folder...
cp $finalcat ../stage1_results/.
cp -r CFHTLS_2013-12-08_10:20:56/* ../stage1_results/

# ======================================================================
# 2013-12-10 (Tuesday) 13:22 GMT

# Test stage 2 labels in database:

cd ../stage2_test

SWIPE.csh stage2_2013-12-03.tar.gz

SWAPSHOP.csh -s CFHTLS --startup --fast --stage2

# No classifications read in - possibly due to name changes? (Parrish)
# Retry with live database tomorrow.

# ======================================================================
# 2013-12-11 (Wednesday) 12:02 GMT

cd ../stage2

SWIPE.csh spacewarp_2013-12-11.gz

SWAPSHOP.csh -s CFHTLS --startup --fast --stage2

# Good - runs OK! Aborted, restarted with printing turned off. Will be slow at
# first as it has to page through all classifications since the_beginning
# - so manually edited startup.config to avouid this (and commented
# out the copy from swap dir in SWAPSHOP)
# Used:
#   start: 2013-12-10_00:00:00

# Hmm - what time zone is this? Would be good to have a more accurate
# start time...

# Anyway, classifications are not being correctly recognised as stage 2 by
# SWAP. Here's an example:

Found classification from different stage:  ('2013-12-11_00:09:55',
'51f5063e0aab2a718b00000c', '5183f151e4bb210219045886', 'ASW00063ra', 'test',
'test', 'NOT', 'UNKNOWN',
'http://spacewarps.org/subjects/standard/5183f151e4bb210219045886.png', 1)

# Switched to taking stage from metadata.

# Looks like classifier.coffee is not labelling classifications correctly:

WARNING: classification labelled stage 1, while subject is stage 2
Found classification from this stage:  ('2013-12-11_10:20:33', '109.30.36.48',
'5183f151e4bb21021900e52b', 'ASW000199n', 'test', 'test', 'NOT', 'UNKNOWN',
'http://spacewarps.org/subjects/standard/5183f151e4bb21021900e52b.png', 2)

# And also, here's a classification of a stage 1 subject!

Found classification from different stage:  ('2013-12-11_10:20:33',
'109.30.36.48', '5183f151e4bb21021904738b', 'ASW000693f', 'training', 'sim',
'NOT', 'LENS',
'http://spacewarps.org/subjects/standard/5183f151e4bb21021904738b.png', 1)

# DAMN. Some training images are still labelled stage 1.
# AND: No classifications are labelled stage 2!
# FUCK. Now we have to use timestamps.

# Re-run and save output, then browse to find first timestamp.

SWAPSHOP.csh -s CFHTLS --startup --fast --stage2 >& swap.log &

# Looks like magic start time was:

# 2013-12-10_13:29:41

# Set this as start time in stage2.config and edit SWAPSHOP to read it

# Now re-run looking at classifications to see if stage is labelled...

\rm -rf CFHTLS_* swap.log update.config

SWAPSHOP.csh -s CFHTLS --startup --fast --stage2 --config stage2.config >& swap.log &

# Ohhhhh it's an ANNOTATION:

# {u'_id': ObjectId('52a759815b6a137dd400021a'), u'user_id':
# ObjectId('50ef25bc6b34c90aab000017'), u'created_at': datetime.datetime(2013,
# 12, 10, 18, 12, 17), u'updated_at': datetime.datetime(2013, 12, 10, 18, 12,
# 17, 343000), u'user_ip': u'94.3.55.144', u'workflow_id':
# ObjectId('528107773ae7400fd4000001'), u'subjects': [{u'zooniverse_id':
# u'ASW0001gfp', u'coords': [], u'id': ObjectId('5183f151e4bb210219010975'),
# u'location': {u'thumbnail':
# u'http://spacewarps.org/subjects/thumbnail/5183f151e4bb210219010975.png',
# u'standard':
# u'http://spacewarps.org/subjects/standard/5183f151e4bb210219010975.png'}}],
# u'subject_ids': [ObjectId('5183f151e4bb210219010975')], u'project_id':
# ObjectId('5101a1341a320ea77f000001'), u'user_name': u'coulditbemanilow',
# u'annotations': [{u'stage': u'2'}, {u'finished_at': u'Tue, 10 Dec 2013
# 18:12:14 GMT', u'started_at': u'Tue, 10 Dec 2013 18:10:59 GMT'},
# {u'user_agent': u'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101
# Firefox/25.0'}, {u'lang': u'en'}]}

# Edit SWAP and try again:

\rm -rf CFHTLS_* swap.log update.config

SWAPSHOP.csh -s CFHTLS --startup --fast --stage2 --config stage2.config >& swap.log &

# Did we get any stage 1 subject warnings?
# Yes:

# WARNING: classification labelled stage 1, while subject is stage 2!
# Found classification from different stage:  1  cf.  2 , items =
# ('2013-12-10_16:30:38', '52a735e65b6a1304ad00005b',
# '5183f151e4bb210219068fd0', 'ASW00097tc', 'test', 'test', 'LENS', 'UNKNOWN',
# 'http://spacewarps.org/subjects/standard/5183f151e4bb210219068fd0.png', '1')

grep WARNING swap.log | wc -l
#      651

# Set SWAP to ignore these completely.

# First correct stage 2 classification is this one:

# Found classification from this stage:  ('2013-12-10_16:47:03',
# '503f1bff0454e20bf40004b3', '5183f151e4bb21021905aa13', 'ASW0007yfn', 'test',
# 'test', 'NOT', 'UNKNOWN',
# 'http://spacewarps.org/subjects/standard/5183f151e4bb21021905aa13.png', '2')

# Use this time in stage2.config instead! 2013-12-10_16:47:00

# Looks like times are GMT! (Or perhaps UTC?)


# OK, need to read things in subject metadata properly, to interpret
# false positives. What words are used?

\rm -rf CFHTLS_* swap.log update.config

SWAPSHOP.csh -s CFHTLS --startup --fast --stage2 --config stage2.config >& swap.log &

# Great. Noticed this was done in hasty mode - want to turn this off,
# to use all classifications and reduce false neg rate! Re-ran, ready for
# production.

# ======================================================================
# 2013-12-12 (Thursday) 12:54 GMT

SWIPE.csh spacewarp_2013-12-12.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2

# Good - first five candidates! Start downloading images, based on
# cands catalog (ie into informatively named files!)

open `grep -v '#'  CFHTLS_2013-12-12_10:23:40/CFHTLS_2013-12-12_10:23:40_candidate_catalog.txt | \
        awk '{if ($2 > 0.95) printf "http://talk.spacewarps.org/#/subjects/%s\n", $1}'

# Add to download script...

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download

# ======================================================================
# 2013-12-13 (Friday) 12:23 GMT

SWIPE.csh spacewarp_2013-12-13.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download

# ======================================================================
# 2013-12-16 (Monday) 11:14 GMT

SWIPE.csh spacewarp_2013-12-15.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download


SWIPE.csh spacewarp_2013-12-16.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download

# ======================================================================
# 2013-12-17 (Tuesday) 12:28 GMT

SWIPE.csh spacewarp_2013-12-17.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download

# ======================================================================
# 2013-12-18 (Wednesday) 14:36 GMT

SWIPE.csh spacewarp_2013-12-18.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download

# ======================================================================
# 2013-12-19 (Thursday) 14:28 GMT

SWIPE.csh spacewarp_2013-12-19.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download

# ======================================================================
# 2013-12-29 (Sunday) 18:30 PST

SWIPE.csh spacewarp_2013-12-29.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download

# ======================================================================
# 2014-01-07 (Tuesday) 17:34 PST

# Been a while, but here's the final stage 2 database. It contains some
# VICS82 classifications, but they are labelled stage 1 so shouldn't feature.
# Turned on stage 1 warning, just to see them!

SWIPE.csh spacewarp_2014-01-07.gz

SWAPSHOP.csh -s CFHTLS --fast --stage2 --download

# Run this before doing VICS82 stuff - doesn't take long.

# Good news - stage flagged OK:

# SWAP: ....................WARNING: classification labelled stage 1, while subject is stage 2!
# WARNING: classification labelled stage 1, while subject is stage 2!
# WARNING: classification labelled stage 1, while subject is stage 2!
# WARNING: classification labelled stage 1, while subject is stage 2!
# WARNING: classification labelled stage 1, while subject is stage 2!
# WARNING: classification labelled stage 1, while subject is stage 2!
# WARNING: classification labelled stage 1, while subject is stage 2!
# WARNING: classification labelled stage 1, while subject is stage 2!
# .........................
# SWAP: total no. of classifications processed:  31034

# However, subsequent runs of SWAP fail for some reason. 0 classifications
# found? Some problem with final timestamp?

# Check out candidates:

ls candidates/*png | wc -l
#       88

# But, some of the downloaded images are empty - maybe because they were
# supposed to be VICS82 ones?! Look at catalog:

du -h candidates/*png | grep 0B
#   0B    candidates/ASW0000ctm.png
#   0B    candidates/ASW000234o.png
#   0B    candidates/ASW0002bmc.png
#   0B    candidates/ASW0002qtn.png
#   0B    candidates/ASW0003mb7.png
#   0B    candidates/ASW00047ae.png
#   0B    candidates/ASW00048x4.png
#   0B    candidates/ASW00059a7.png
#   0B    candidates/ASW0005kad.png
#   0B    candidates/ASW0005lmz.png
#   0B    candidates/ASW0005u1n.png
#   0B    candidates/ASW00062bc.png
#   0B    candidates/ASW0006e0o.png
#   0B    candidates/ASW0006ker.png
#   0B    candidates/ASW0006ksp.png
#   0B    candidates/ASW0006ktg.png
#   0B    candidates/ASW00072kz.png
#   0B    candidates/ASW00078we.png
#   0B    candidates/ASW0007e08.png
#   0B    candidates/ASW0007sez.png
#   0B    candidates/ASW0007u3n.png
#   0B    candidates/ASW0007uda.png
#   0B    candidates/ASW0007vx2.png
#   0B    candidates/ASW0008mtv.png
#   0B    candidates/ASW0009a68.png
#   0B    candidates/ASW0009bq9.png
#   0B    candidates/ASW0009coy.png

# check talk...

ASW0009coy is CFHTLS
ASW0006ker is CFHTLS

# Probably these are all stage 2 candidates, but server is refusing image
# requests... check logs:

more candidates/.ASW0009coy.log
# --2013-12-29 19:44:04--  http://spacewarps.org/subjects/standard/5183f151e4bb21021906a882.png
# Resolving spacewarps.org... failed: nodename nor servname provided, or not known.
# wget: unable to resolve host address `spacewarps.org'

# Whoah! Tested wgetting one, and it worked OK. Phew. Need to redownload
# these, preferably by re-running SWAP sometime...

# Now though, need to focus on VICS82!
# Start downloading new dump, try reading VICS82 classifications.

# ======================================================================

# Running list of CFHTLS stage 2 candidates!

# Select at P > 0.95
#
# 43 candidates, most of them known, one or two false positives, some new lenses!

# Good candidates:
http://talk.spacewarps.org/#/subjects/ASW0007xrs   # Commenters disagree
http://talk.spacewarps.org/#/subjects/ASW0007h27   # 2 possible lenses
http://talk.spacewarps.org/#/subjects/ASW0005o38   # Nice galaxy-scale arc
http://talk.spacewarps.org/#/subjects/ASW0004pbz   # Tiny binary lens
http://talk.spacewarps.org/#/subjects/ASW0004nan   # Bright UV arc under LRG
http://talk.spacewarps.org/#/subjects/ASW00008a0   # Thin arcs, multiple imaging
http://talk.spacewarps.org/#/subjects/ASW0009bbq   # Faint cluster arc
http://talk.spacewarps.org/#/subjects/ASW00096rm   # Nice tiny quad
http://talk.spacewarps.org/#/subjects/ASW000993q   # Cluster lens, plus galaxy pertuber
http://talk.spacewarps.org/#/subjects/ASW0006i2r   # Nice arc next to LRG
http://talk.spacewarps.org/#/subjects/ASW0004106   # Sheared arc! 3-plane lens :-)
http://talk.spacewarps.org/#/subjects/ASW0005vog   # Binary lens, faint arc with knot

# False positives:
http://talk.spacewarps.org/#/subjects/ASW0003ox0   # Dashboard shows it to be a merging spiral
http://talk.spacewarps.org/#/subjects/ASW000279i   # Nearby dwarf with companions
http://talk.spacewarps.org/#/subjects/ASW00032ae   # 3-way merger, plus orange/blue spiral
http://talk.spacewarps.org/#/subjects/ASW00011ls   # Spiral/Ring?

# Unknown:
http://talk.spacewarps.org/#/subjects/ASW0001rcc   # Wierd g-band circular blob, bigger than i/r
http://talk.spacewarps.org/#/subjects/ASW0006noe   # Promising but needs more work
http://talk.spacewarps.org/#/subjects/ASW0009ans   # Bright UV arc, 2 or 3 blobs?
http://talk.spacewarps.org/#/subjects/ASW0000hng   # Odd U-shaped arc, v bright star nearby
http://talk.spacewarps.org/#/subjects/ASW00024id   # Weird high radius arc, ring galaxy?

# ======================================================================
# 2014-01-30

# Zooming in on the good candidates:

set goods = `grep 'http://talk' ../../doc/notes/notes-pjm.txt | grep '# ' | head -12 | cut -d'/' -f6 | awk '{printf "candidates/%s.png\n", $1}' | sort`

mkdir goods
cp $goods goods/.

# Now zooms! Estimate fractional central x and y from preview�

set images = ( \
ASW0004nan.png \
ASW0004pbz.png \
ASW0005o38.png \
ASW0004dv8.png \
ASW0006i2r.png \
ASW0007h27.png \
ASW0007xrs.png \
ASW00008a0.png \
ASW0009bbq.png \
ASW00096rm.png \
ASW000993q.png \
ASW0004106.png \
)

set x = (\
0.75 \
0.7 \
0.7 \
0.45 \
0.8 \
0.35 \
0.2 \
0.75 \
0.6 \
0.75 \
0.55 \
0.45 \
)

set y = ( \
0.65 \
0.45 \
0.3 \
0.8 \
0.75 \
0.85 \
0.87 \
0.2 \
0.85 \
0.8 \
0.75 \
0.7 \
)

foreach k ( `seq $#images` )
  set image = goods/$images[$k]
  set zoom = ${image:r}_zoom.png
  set i = `echo $x[$k] | awk '{printf "%d\n", $1*440 - 50}'`
  set j = `echo $y[$k] | awk '{printf "%d\n", $1*440 - 50}'`
  set string = "100x100+$i+$j"
  convert -crop $string $image $zoom
  du -h $zoom
end

# Good!

# ======================================================================
VICS82START:
# ======================================================================
# 2014-01-07 (Tuesday) 21:18 PST

# Run SWAP in VICS82/stage1 directory - testing for project!

SWAPSHOP.csh -s VICS82 --startup --fast --config stage1.config

# Config file copied from CFHTLS stage 2 dir - don't be hasty,
# set initial date to the beginning of the week.

# Fail - 0 classifications found etc. Maybe mongo is broken?
# OK, trash old mongo and swipe the new one. Gulp! Can always just start from
# half an hour ago for testing... First, rerun verbose, see whats happening?

# oops, all classifications have items=None, must have screwed up somewhere.
# To do one by one test, need to run python directly:

SWAP.py stage1.config

# OK, fixed bugs, survey vs project comparison now in operation.

# Good! Seems to be working OK. Test classifications (not on live site)
# mostly junk, see report in attic. Onwards!

# ======================================================================
# 2014-01-07 (Tuesday) 22:53 PST

# OK, start the agents at 20:00 GMT for the start of Stargazing Live:
# Need to trash mongo in old CFHTLS stage 2 dir.
# Do 5 million at a time for speed!

SWIPE.csh spacewarp_2014-01-08.gz

SWAPSHOP.csh -s VICS82 --startup --fast --config stage1.config

# Wed Jan  8 01:14:32 PST 2014
# SWAP: updating all subjects with classifications made since 2014-01-07_20:00:00
# SWAP: interpreting up to 5000000  classifications...
# SWAP: ..................
# SWAP: total no. of classifications processed:  1235829
# ...
# SWAP: saving retiree subject Zooniverse IDs...
# SWAP: 2421 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/VICS82/stage1/VICS82_2014-01-08_01:20:51/VICS82_2014-01-08_01:20:51_retire_these.txt
# SWAP: saving lens candidates...
# SWAP: 24 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/VICS82/stage1/VICS82_2014-01-08_01:20:51/VICS82_2014-01-08_01:20:51_candidates.txt
# ...
# SWAP: From 11471 subjects classified,
# SWAP: 8654 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/VICS82/stage1/VICS82_2014-01-08_01:20:51/VICS82_2014-01-08_01:20:51_candidate_catalog.txt
# Wed Jan  8 02:04:03 PST 2014

# SWAP takes about 50 mins to do 1.2 million classifications. 40 mins per
# million.

# Only 11k subjects served?! DB problem?

# Low number of retirees; cascade plot shows a lot of images with high
# uncertainty, hanging out in the middle.

# 24 candidates with P > 0.95 - need to check these out!

SWAPSHOP.csh -s VICS82 --startup --fast --no-analysis --download

# SWAPSHOP: in folder 'candidates',
# SWAPSHOP: downloading 24 images from
# ../VICS82_2014-01-08_01:20:51/VICS82_2014-01-08_01:20:51_candidate_catalog.txt...........................SWAPSHOP:
# ...done.
mv candidates candidates_P.gt.0.95

# Hmm - one or two mildly interesting but nothing remarkable. Try digging
# deeper, P > 0.10

# SWAPSHOP: downloading 87 images from ../VICS82_2014-01-08_01:20:51/VICS82_2014-01-08_01:20:51_candidate_catalog.txt

mv candidates candidates_P.gt.0.10

# Not much to be gained! Made galleries:

gallery.pl -pdf -x 1 -y 1 -t -o VICS82_1stmillion_P.gt.0.95.pdf \
  candidates_P.gt.0.95/*.png

gallery.pl -pdf -x 1 -y 1 -t -o VICS82_1stmillion_P.gt.0.10.pdf \
  candidates_P.gt.0.10/*.png

# Focus on P > 0.95.
# Highlights:

Cluster with several candidate lensed images; no obvious conjugate systems
but some images are very red:

  gallery.pl -pdf -x 1 -y 1 -t -o VICS82_1stmillion_Cluster.pdf \
    candidates_P.gt.0.10/ASW0009i4s.png

Several nearly-lensed quasars: quasars close to massive galaxies, some
possibly with faint counter-images buried in lens galaxy.

  gallery.pl -pdf -x 2 -y 2 -t -o VICS82_1stmillion_4NLQs.pdf \
    candidates_P.gt.0.10/ASW0009kg3.png \
    candidates_P.gt.0.10/ASW0009kj8.png \
    candidates_P.gt.0.10/ASW0009okd.png \
    candidates_P.gt.0.10/ASW0009pbd.png

Possible long blue arc:

  gallery.pl -pdf -x 1 -y 1 -t -o VICS82_1stmillion_LongArc.pdf \
    candidates_P.gt.0.10/ASW0009jxt.png

Short red arc:

  gallery.pl -pdf -x 1 -y 1 -t -o VICS82_1stmillion_ShortRedArc.pdf \
    candidates_P.gt.0.10/ASW0009jlw.png


# Miscellaneous, not yet SWAP or Talk-detected:

Double purple quasar?
   http://talk.spacewarps.org/#/subjects/ASW0009lgi

Aprajita's bright double:
   http://talk.spacewarps.org/#/subjects/ASW0009uo2

Jim's eyeball ring:
   http://talk.spacewarps.org/#/subjects/ASW0009io9
   02:09:41.3 +00:15:58.7

# BTW what else is nearby?

askSDSS -f -w 1 --hms 02:09:41.3 +00:15:58.7
# askSDSS: query the SDSS website for images, catalogues and spectra
# askSDSS: required position: 32.42208 0.26631
# askSDSS: TODO: make finding chart
# askSDSS: field size / arcmin: 1
# askSDSS: ra (J2000): 02 09 41.3
# askSDSS: dec (J2000): +00 15 58.7
# askSDSS: IAU name of search position: SDSSJ020941.3+001558.7
# askSDSS: downloading image...
# askSDSS: finding chart image:
#   SDSSJ020941.3+001558.7_1x1arcmin.jpg
# askSDSS: Explorer URL:
#   http://skyserver.sdss3.org/public/en/tools/chart/chart.asp?ra=32.42208&dec=0.26631&opt=G&
# askSDSS: all done

# Not much! :-) Here's the 40" SDSS view:

askSDSS -f -w 0.67 --hms 02:09:41.3 +00:15:58.7

# ======================================================================
# 2014-01-07 (Tuesday) 22:53 PST

# Second run, on 3 million classifications. Over-wrote tarball, so trash mongo
# and old dir.

\rm -rf spacewarp_2014-01-08 mongo

SWIPE.csh spacewarp_2014-01-08.gz

SWAPSHOP.csh -s VICS82 --fast --download

# Started swiping at 0415. If my estimate of 3 hours is correct, it shoudl be
# done by 0800 PST = 1600 GMT
# Finished at 0730 PST. Good!

# First, check for 0209:
grep ASW0009io9 VICS82_2014-01-08_10:35:11/VICS82_2014-01-08_10:35:11_candidate_catalog.txt
# ASW0009io9  0.0000575  3       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019e4.png

# Bizarre - P < P0 by a factor of 3. Oh well, only 3 classifications.

wget -O ASW0009io9.png http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019e4.png

# Now then, candidates:

SWAPSHOP: in folder 'candidates',
SWAPSHOP: downloading 52 images from ../VICS82_2014-01-08_10:35:11/VICS82_2014-01-08_10:35:11_candidate_catalog.txt

mv candidates VICS82_2million_candidates_P.gt.0.95
gallery.pl -pdf -x 1 -y 1 -t -o VICS82_2million_candidates_P.gt.0.95.pdf \
  VICS82_2million_candidates_P.gt.0.95/*.png

# Nice arc:

VICS82_2million_candidates_P.gt.0.95/ASW0009dqb.png

# Copy report:

cp VICS82_stage1_report.pdf VICS82_2million_report.pdf

# Cascade looks *much* better! Some good rejection going on, and 39804
# subjecst now classified. Try pulling out systems above 1%:

SWAPSHOP.csh -s VICS82 --fast --no-analysis --download

# 398 images! Cool. Browse these.
mv candidates VICS82_2million_candidates_P.gt.0.01
gallery.pl -pdf -x 2 -y 2 -t -o VICS82_2million_candidates_P.gt.0.01.pdf \
  VICS82_2million_candidates_P.gt.0.01/*.png

# Quite like this one:
ASW0009iov.png

# Grab these two:
  gallery.pl -pdf -x 1 -y 1 -t -o VICS82_2million_NiceArcs.pdf \
    VICS82_2million_candidates_P.gt.0.95/ASW0009dqb.png \
    VICS82_2million_candidates_P.gt.0.01/ASW0009iov.png

# From Christine: "2 red dots"
http://talk.spacewarps.org/#/subjects/ASW0009i91

# From Claude:
http://talk.spacewarps.org/#/subjects/ASW0009dk6

head -1 \
  VICS82_2014-01-08_10:35:11/VICS82_2014-01-08_10:35:11_candidate_catalog.txt ; \
grep -e ASW0009dqb -e ASW0009iov -e ASW0009io9 \
     -e ASW0009i91 -e ASW0009dk6 \
  VICS82_2014-01-08_10:35:11/VICS82_2014-01-08_10:35:11_candidate_catalog.txt

# # zooid     P          Nclass  image
# ASW0009dqb  1.0000000  279       http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c0000de.png
# ASW0009i91  0.0000306  7       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0017c0.png
# ASW0009dk6  0.0007486  7       http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c000001.png
# ASW0009io9  0.0000575  3       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019e4.png
# ASW0009iov  0.0128811  46       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019fa.png

wget -O ASW0009dk6.png http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c000001.png
wget -O ASW0009i91.png http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0017c0.png

# Retirements?

wc -l VICS82_2million_retire_these.txt
#     5745 VICS82_2million_retire_these.txt

# Sent to Michael.

# Best arcs for lenshunters:
  gallery.pl -pdf -x 1 -y 1 -t -o VICS82_2million_best.pdf \
    ASW0009io9.png \
    VICS82_2million_candidates_P.gt.0.95/ASW0009dqb.png \
    VICS82_2million_candidates_P.gt.0.01/ASW0009iov.png


# OK, on with the next batch!

# ======================================================================
# 2014-01-08 (Wednesday) 08:55 PST

\rm -rf spacewarp_2014-01-08 mongo

SWIPE.csh spacewarp_2014-01-08.gz

SWAPSHOP.csh -s VICS82 --fast --download

Summary of output:

SWAP: total no. of classifications processed:  338074
SWAP: saving lens candidates...
SWAP: 69 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/VICS82/stage1/VICS82_2014-01-08_15:29:15/VICS82_2014-01-08_15:29:15_candidates.txt
SWAPSHOP: if you want, you can go ahead and retire 10656 subjects with

# Cool. Report shows 2.7 million classifications of 41538 subjects by
# 38k users, 69 candidates, 18725 rejections. Plots very encouraging.

mv VICS82_stage1_report.pdf VICS82_2.7million_report.pdf
mv VICS82_stage1_retire_these.txt VICS82_2.7million_retire_these.txt
wc -l VICS82_2.7million_retire_these.txt
#   10656 VICS82_2.7million_retire_these.txt

# Quick look for new candidates:

mv candidates VICS82_2.7million_candidates_P.gt.0.95
gallery.pl -pdf -x 1 -y 1 -t -o VICS82_2.7million_candidates_P.gt.0.95.pdf \
  VICS82_2.7million_candidates_P.gt.0.95/*.png

# YES! 9io9 detected :-)
   http://talk.spacewarps.org/#/subjects/ASW0009io9

# Possibly interesting blue arcs:
   http://talk.spacewarps.org/#/subjects/ASW000a8ep

# Anu reports this one from Talk:
   http://talk.spacewarps.org/#/subjects/ASW0009l59
#   00:39:08.6 -00:58:28.1

head -1 \
   VICS82_2014-01-08_15:29:15/VICS82_2014-01-08_15:29:15_candidate_catalog.txt ; \
grep -e ASW0009io9 -e ASW0009l59 \
   VICS82_2014-01-08_15:29:15/VICS82_2014-01-08_15:29:15_candidate_catalog.txt

# # zooid     P          Nclass  image
# ASW0009l59  0.0000064  4       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c002668.png
# ASW0009io9  0.9640383  13       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019e4.png

# Again, 9l59 needs more classifications. Talk FTW!

# For now, make gallery of all best candidates - I count 6:

head -1 \
   VICS82_2014-01-08_15:29:15/VICS82_2014-01-08_15:29:15_candidate_catalog.txt ; \
grep -e ASW0009dqb \
     -e ASW0009iov \
     -e ASW0009io9 \
     -e ASW0009i91 \
     -e ASW0009dk6 \
     -e ASW0009l59 \
   VICS82_2014-01-08_15:29:15/VICS82_2014-01-08_15:29:15_candidate_catalog.txt

# # zooid     P          Nclass  image
# ASW0009dqb  1.0000000  288      http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c0000de.png
# ASW0009i91  0.0183210  16       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0017c0.png
# ASW0009dk6  0.5566131  11       http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c000001.png
# ASW0009l59  0.0000064  4        http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c002668.png
# ASW0009io9  0.9640383  13       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019e4.png
# ASW0009iov  0.0053120  51       http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019fa.png

wget -O ASW0009dqb.png http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c0000de.png
wget -O ASW0009iov.png http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019fa.png
wget -O ASW0009l59.png http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c002668.png


# Download these
  gallery.pl -pdf -x 1 -y 1 -t -o VICS82_2.7million_Top5.pdf \
    ASW0009io9.png \
    ASW0009l59.png \
    ASW0009iov.png \
    ASW0009dqb.png \
    ASW0009dk6.png

# ======================================================================
# 2014-01-08 (Wednesday) 20:42 PST

\rm -rf spacewarp_2014-01-08*

SWIPE.csh spacewarp_2014-01-09.gz

SWAPSHOP.csh -s VICS82 --fast --download

# Whoah - db is 21GB! SWAP runs very slowly - or even gets stuck on
# db.find()? No - it couldnt connect to the mongo...
# RE-swipe! God-damnint. OK scratch that, got a new dump.


# ======================================================================
# 2014-01-09 (Thursday) 05:39 PST

SWIPE.csh spacewarp_2014-01-09.gz

SWAPSHOP.csh -s VICS82 --fast --download

# OK, what have we got here?

SWAP: total no. of classifications processed:  2449799
# +2.7million = 5.1 million
# 55609 classification agents, 41998 subjects

SWAP: saving retiree subject Zooniverse IDs...
SWAP: 36791 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/VICS82/stage1/VICS82_2014-01-09_11:00:00/VICS82_2014-01-09_11:00:00_retire_these.txt
SWAP: saving lens candidates...
SWAP: 153 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/VICS82/stage1/VICS82_2014-01-09_11:00:00/VICS82_2014-01-09_11:00:00_candidates.txt

# Cool! 37k subjects to be retired in total, 28655 still to do.
# Report:

mv VICS82_stage1_report.pdf VICS82_5.1million_report.pdf
mv VICS82_stage1_retire_these.txt VICS82_5.1million_retire_these.txt

# 153 P.gt.0.95 candidates! Including some new ones, pulled these out by hand:

mv candidates VICS82_5.1million_candidates_P.gt.0.95

gallery.pl -pdf -x 2 -y 2 -t -o VICS82_5.1million_candidates_P.gt.0.95.pdf \
  VICS82_5.1million_candidates_P.gt.0.95/ASW*png

# Nice little red arc?
  http://talk.spacewarps.org/#/subjects/ASW000a2y7

# Blue fuzz under pair:
  http://talk.spacewarps.org/#/subjects/ASW0009dvs

# A little double?
  http://talk.spacewarps.org/#/subjects/ASW0009fht

# Little red arc? With extent plus counterimage?
  http://talk.spacewarps.org/#/subjects/ASW0009ifa

# Pink quad?
  http://talk.spacewarps.org/#/subjects/ASW0009klz

# Yellow! With blue fuzz underneath.
  http://talk.spacewarps.org/#/subjects/ASW0009vfi

# Combine with others:

head -1 \
  VICS82_2014-01-09_11:00:00/VICS82_2014-01-09_11:00:00_candidate_catalog.txt ; \
grep \
-e ASW0009io9 \
-e ASW0009l59 \
-e ASW0009iov \
-e ASW0009dqb \
-e ASW0009dk6 \
-e ASW000a2y7 \
-e ASW0009dvs \
-e ASW0009fht \
-e ASW0009ifa \
-e ASW0009klz \
-e ASW0009vfi \
  VICS82_2014-01-09_11:00:00/VICS82_2014-01-09_11:00:00_candidate_catalog.txt

# zooid     P          Nclass  image
ASW0009klz  0.9999979  24      http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0023b2.png
ASW0009dqb  1.0000000  288     http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c0000de.png
ASW0009dk6  0.9998838  46      http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c000001.png
ASW0009ifa  1.0000000  132     http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0018a1.png
ASW0009l59  1.0000000  363     http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c002668.png
ASW0009fht  1.0000000  119     http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c0009cc.png
ASW0009vfi  0.9844575  25      http://spacewarps.org/subjects/standard/52c1c4d03ae740492c005a79.png
ASW0009io9  1.0000000  51      http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019e4.png
ASW000a2y7  0.9999997  108     http://spacewarps.org/subjects/standard/52c1c4d03ae740492c00808a.png
ASW0009dvs  0.9959316  43      http://spacewarps.org/subjects/standard/52c1c4ce3ae740492c0001a3.png
ASW0009iov  0.9999985  127     http://spacewarps.org/subjects/standard/52c1c4cf3ae740492c0019fa.png

# Check out those probabilities!!
# OK, make gallery:

gallery.pl -pdf -x 1 -y 1 -t -o VICS82_5.1million_Top11.pdf \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009io9.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009l59.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009klz.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009dqb.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009dk6.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009ifa.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009fht.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009vfi.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW000a2y7.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009dvs.png \
   VICS82_5.1million_candidates_P.gt.0.95/ASW0009iov.png

# OK, send all to team.

# ======================================================================
# 2014-01-14 (Tuesday) 06:48 PST

\rm -rf spacewarp_2014-01-09*

SWIPE.csh spacewarp_2014-01-14.gz

SWAPSHOP.csh -s VICS82 --fast --download

# From report:

# 7.3 million classifications, 71894 classifiers, 196 candidates.

mv VICS82_stage1_report.pdf VICS82_7.3million_report.pdf
mv candidates VICS82_7.3million_candidates_P.gt.0.95
mv VICS82_stage1_retire_these.txt VICS82_7.3million_retire_these.txt

# Gallery of 196 candidates:
gallery.pl -pdf -x 2 -y 2 -t -o VICS82_7.3million_candidates_P.gt.0.95.pdf \
  VICS82_7.3million_candidates_P.gt.0.95/ASW*png

# Nice, but short, red arcs:
  http://talk.spacewarps.org/#/subjects/ASW0009hu7

# That's about it.

# ======================================================================
# 2014-01-15 (Wednesday) 10:01 PST

# Is this the last SWAP run?

\rm -rf spacewarp_2014-01-14*

SWIPE.csh spacewarp_2014-01-15.gz

SWAPSHOP.csh -s VICS82 --fast --download

# From report:

# 7.3 million classifications, 71895 classifiers, 196 candidates.
# Looks like its converged.

mkdir -p 7.3million
mv VICS82_stage1_report.pdf 7.3million/VICS82_7.3million_report.pdf
mv candidates 7.3million/VICS82_7.3million_candidates_P.gt.0.95
mv VICS82_stage1_retire_these.txt 7.3million/VICS82_7.3million_retire_these.txt

# Gallery of 196 candidates:
gallery.pl -pdf -x 2 -y 2 -t -o 7.3million/VICS82_7.3million_candidates_P.gt.0.95.pdf \
  7.3million/VICS82_7.3million_candidates_P.gt.0.95/ASW*png

# Nice, but short, red arcs:
  http://talk.spacewarps.org/#/subjects/ASW0009hu7

# Check candidate catalog:

cp VICS82_2014-01-15_09:36:47/VICS82_2014-01-15_09:36:47_candidate_catalog.txt \
   7.3million/VICS82_7.3million_candidate_catalog.txt

# Only 19 have fewer than 20 classificatiosn, only 1 has less than 10.

# ======================================================================
# 2014-02-28 (Friday) 17:59 PST

# OK, get IDs for CFHTLS stage 2 candidates, P > 0.95.

# Search back in notes, just before "VICS82START:"

# In stage2 directory, take most recent catalog of candidates and
# threshold... Some systems have low prob because they have not been
# classified enough, unfortunately.

set LASTANALYSIS = CFHTLS_2013-12-29_10:22:57

cp ${LASTANALYSIS}/${LASTANALYSIS}_candidate_catalog.txt \
   CFHTLS_stage2_candidate_catalog.txt

set master = CFHTLS_stage2_candidate_catalog.txt
set p95cat = ${master:r}_p.gt.0.95.txt

grep -e '#' $master > $p95cat
grep -v '#' $master | \
  awk '{if ($2 > 0.95) print $0}' >> $p95cat

# 84 out of a possible 1419 stage 2 targets.

# ======================================================================
# 2014-03-07 (Friday) 10:29 CST

# Experimenting with unsupervised learning, in VICS82/unsupervised
# directory. Do a three-way comparison with the first 10^6 classifications
# only. Just run SWAP once, with batch of 10^6.

set test_dir = $SWAP_DIR/VICS82/stage1/supervision_tests
mkdirf $test_dir
cp $SWAP_DIR/VICS82/stage1/stage1.config template.config

# Edit this, to include learning = supervised etc.

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# EXPERIMENT 1 - standard operation

# supervised: True
# initialPL: 0.5
# initialPD: 0.5

set expt = 'supervised_random'
mkdirf ${test_dir}/${expt}
cp ${test_dir}/template.config ${expt}_stage1.config

SWAP.py ${expt}_stage1.config

# VICS82 terrible data? Seems like a lot of test subjects drifting up...
# Need to compare with old code.

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# EXPERIMENT 2 - supervised, agents assume astute volunteers

# supervised: True
# initialPL: 0.75
# initialPD: 0.75

set expt = 'supervised_astute'
mkdirf ${test_dir}/${expt}
cp ${test_dir}/template.config ${expt}_stage1.config

SWAP.py ${expt}_stage1.config

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# EXPERIMENT 3 - unsupervised, agents assume astute volunteers

# supervised: False
# initialPL: 0.75
# initialPD: 0.75

set expt = 'unsupervised_astute'
mkdirf ${test_dir}/${expt}
cp ${test_dir}/template.config ${expt}_stage1.config

SWAP.py ${expt}_stage1.config

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# EXPERIMENT 4 - unsupervised, agents assume random volunteers

# supervised: False
# initialPL: 0.5
# initialPD: 0.5

set expt = 'unsupervised_random'
mkdirf ${test_dir}/${expt}
cp ${test_dir}/template.config ${expt}_stage1.config

SWAP.py ${expt}_stage1.config

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Switched to unsupervised branch before breaking the code!!

# Interesting!
# * Astute, unsupervised agents are quicker to reject subjects,
#   as they are continuously reinforced in the "lenses are rare" mantra
# * These agents' PL values do not change much, because they don't see
#   many lenses.
# * Random, unsupervised agents stay random in PL - and do worst of all
#   in lens detection?


# ======================================================================
# 2014-05-29 (Thursday) 17:45 PDT

# Housekeeping - put all analysis subdirs in workspace, where they are ignored
# by git...

# ======================================================================
# 2014-05-28 (Wednesday) 18:07 PDT

# Pull request from Surhud:
#
#     I have added a random number seed to SWAPSHOP so that the results are reproducible.
#
#     Changes are as follows:
#
#     1) Add an extra argument to SWAPSHOP.csh which takes as input a random
#        seed. However it only works when startup is specified.
#     2) SWAPSHOP.csh runs the program generate_random_state.py to generate a
#        random state file from the seed provided by the user.
#     3) The config files have an extra compulsory option (random_file) which is
#        the name of the file which stores the pickled random state.
#     4) SWAP.py stores the random_state in a the random_file at the end of a
#        run when update.config has to be written.

# Test on VICS82, in stage1 directory (linked to from new test dir):

mkdirf VICS82/random-test
ln -s ../stage1/mongo .
ln -s ../stage1/spacewarp_2014-01-15 .
cp ../stage1/stage1.config VICS82_stage1.config

# Edit config file to include random_state pickle...

# OK, restart mongo:

SWIPE.csh spacewarp_2014-01-15.gz

# OMG, mongorestore takes ages!

# Now try running all of VICS82, with 5 million per batch:

SWAPSHOP.csh -s VICS82 --startup --fast --config VICS82_stage1.config

# Hmm - no classifications found. Something wrong with mongo.
# Tear it down and start again, might as well run on CFHTLS then, using
# latest downloaded db. Keep VICS82_stage1.config in new projects dir.

# OK, re-doing CFHTLS stage 1:

cp workspace/CFHTLS/hasty/startup.config projects/CFHTLS_stage1.config

# Edit to include random file stuff... and also stage = 1(!)

goto CFHTLS_STAGE1

# ...

# Did everything finish correctly, does it run repeatably?

# SWAP: From 438654 subjects classified,
# SWAP: 7580 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-05-05_20:30:12/CFHTLS_2014-05-05_20:30:12_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 438654 subjects classified,
# SWAP: 5529 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-05-05_20:30:12/CFHTLS_2014-05-05_20:30:12_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 438654 subjects classified,
# SWAP: 257 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-05-05_20:30:12/CFHTLS_2014-05-05_20:30:12_dud_catalog.txt

# Hmm - 44 candidates, 7580 with P > rejection - this is
# different to previous run! Are stage 2 classifications being used?
# Last date (on trajectory plot) is 2014-05-05_20:30:04, which is strange.
# Plot shows a spike at P = P0, about 4500 high. What are these subjects?

# Looks like there's some junk in the db, from somewhere...
# Need to set an end date for the survey, for now let's say 2014-01-07_23:59:59

# Test this code, just using December's classifications.
# First, archive the problem run:

mkdir -p too-many-subjects-44-candidates
mv CFHTLS* random_state.pickle update.config too-many-subjects-44-candidates/

SWAPSHOP.csh -s CFHTLS --startup --fast --config CFHTLS_stage1_test.config

# Needed to fix time, strings and datetimes... OK!

# SWAP: updating all subjects classified between 2013-12-01_00:00:01
# SWAP: and 2014-01-07_23:59:59
# SWAP: total no. of classifications processed:  159464
# SWAP: From 46736 subjects classified,
# SWAP: 34374 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 46736 subjects classified,
# SWAP: 4354 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 46736 subjects classified,
# SWAP: 3439 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_dud_catalog.txt

# Final date (on plots) is 2014-01-08_00:00:29
# Good!

# Do reproduction test with Anu: do all of CFHTLS stage 1, and compare candidate catalogs.
# Merge Chris and Mike in first, though - to get the new information stuff.

# Merge into master, push and close Surhud's pull request.

# ======================================================================
# 2014-05-30 (Friday) 12:40 PDT

# Merging in Chris and Mike's information and plotting code:

git remote add git@github.com:cpadavis/SpaceWarps.git chris
git checkout -b chris
git pull chris master

# Did this last week. Now need to merge in Surhud's stuff from master, and resolve conflicts:

git merge master

# OK, fixed conflicts. Compare test outputs with previous test run!

mkdir -p test_old-bureaucracy
mv CFHTLS* random_state.pickle update.config test_old-bureaucracy/

cp ../../../projects/CFHTLS_stage1.config CFHTLS_stage1_test.config
# Edit to have same start time as old-bureacracy config, start: 2013-12-01_00:00:01

SWAPSHOP.csh -s CFHTLS --startup --fast --config CFHTLS_stage1_test.config

# SWAP: updating all subjects classified between 2013-12-01_00:00:01
# SWAP: and 2014-01-07_23:59:59
# SWAP: total no. of classifications processed:  159464
# SWAP: From 46736 subjects classified,
# SWAP: 34385 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_candidate_catalog.txt
# SWAP: From 46736 subjects classified,
# SWAP: 4354 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_sim_catalog.txt
# SWAP: From 46736 subjects classified,
# SWAP: 3434 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_dud_catalog.txt

# Hmm: small differences in catalogs. Surhud-only had 34374 candidates (with P > rejection)
# Is this just the random_state? Both were run with no seed (is seed = 7368)

# Run again to test! Made some tweaks to plotting as well.

mkdir -p test_new-bureaucracy_run1
mv CFHTLS* random_state.pickle update.config test_new-bureaucracy_run1/

cp test_new-bureaucracy_run1/CFHTLS_stage1_test.config .

SWAPSHOP.csh -s CFHTLS --startup --fast --config CFHTLS_stage1_test.config

# SWAP: updating all subjects classified between 2013-12-01_00:00:01
# SWAP: and 2014-01-07_23:59:59
# SWAP: total no. of classifications processed:  159464
# SWAP: From 46736 subjects classified,
# SWAP: 34363 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_candidate_catalog.txt
# SWAP: From 46736 subjects classified,
# SWAP: 4354 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 46736 subjects classified,
# SWAP: 3437 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_dud_catalog.txt

# OK, different again. random_state not working!
# Try NOT over-writing random_state. Should be better?
# It doesn't matter if we always use same random state at beginning.

# run3: constant random_state, plus tweaks to plots (skill not contribution in agent probs plot).

mkdir -p test_new-bureaucracy_run2
mv CFHTLS* random_state.pickle update.config test_new-bureaucracy_run2/

cp test_new-bureaucracy_run1/CFHTLS_stage1_test.config .

SWAPSHOP.csh -s CFHTLS --startup --fast --config CFHTLS_stage1_test.config

# SWAP: From 46736 subjects classified,
# SWAP: 34372 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 46736 subjects classified,
# SWAP: 4354 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 46736 subjects classified,
# SWAP: 3437 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_dud_catalog.txt


# Now do a run 4 that is identical, to check numbers. Fixed bug in bureau to keep track of agent skills.

mkdir -p test_new-bureaucracy_run3
mv CFHTLS* random_state.pickle update.config test_new-bureaucracy_run3/

cp test_new-bureaucracy_run1/CFHTLS_stage1_test.config .

SWAPSHOP.csh -s CFHTLS --startup --fast --config CFHTLS_stage1_test.config

# SWAP: From 46736 subjects classified,
# SWAP: 34361 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 46736 subjects classified,
# SWAP: 4354 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 46736 subjects classified,
# SWAP: 3436 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2014-01-08_00:00:29/CFHTLS_2014-01-08_00:00:29_dud_catalog.txt

# Damn. Numbers still inconsistent. Some problem with random_state. Punt to Surhud!

mkdir -p test_new-bureaucracy_run4
mv CFHTLS* random_state.pickle update.config test_new-bureaucracy_run4/

# Check in SWAP code. Changed N_per_batch to 3million to get some rough
# sense of how crowd, sample changes with time.

# Edited end time in config file, to match start time of stage 2:

# start: the_beginning
# end: 2013-12-10_16:47:00

# Add this to the CFHTLS_STAGE1 notes block above.

# Merge back in to master branch, and push so that Surhud and Anu can
# test / fix bugs in random state setting.

# ======================================================================
# 2014-05-30 (Friday) 15:38 PDT

# Now need to run all the way through stage 1 and stage 2 to make good
# bureaus for crowd plots, and to get final samples of candidates.

# Standard runs, for various projects and their reproducibility.

# ----------------------------------------------------------------------
CFHTLS_STAGE1:

# Set up workspace:
set workspace = $SWAP_DIR/workspace/CFHTLS/stage1
mkdir -p $workspace
cd $workspace

# Get the configuration file for this project:
cp $SWAP_DIR/projects/CFHTLS_stage1.config .

# start: the_beginning
# end: 2013-12-10_13:29:41

# Download the db dump if necessary:
set datadir = spacewarp_2014-05-29
if (! -e spacewarp_2014-05-29) then
  wget -O spacewarp_2014-05-29.tar.gz \
    "https://zooniverse-code.s3.amazonaws.com/databases/2014-05-29/ouroboros_projects/spacewarp_2014-05-29.tar.gz?AWSAccessKeyId=AKIAJHHZ7KLFECQKTS7A&Expires=1401968753&Signature=FDzTX4QX1pFjcwZ1tiGD05UDc0s%3D"
  SWIPE.csh spacewarp_2014-05-29.tar.gz
endif

# Run SWAP, save plots:
SWAPSHOP.csh -s CFHTLS --startup --config CFHTLS_stage1.config

# ----------------------------------------------------------------------

CFHTLS_STAGE2:

# Set up workspace:
set workspace = $SWAP_DIR/workspace/CFHTLS/stage2
mkdir -p $workspace
cd $workspace

# Get the configuration file for this project:
cp $SWAP_DIR/projects/CFHTLS_stage2.config .

# start: 2013-12-10_13:29:41
# end: 2014-01-06_23:59:59

# Run SWAP, save plots:
SWAPSHOP.csh -s CFHTLS --startup --config CFHTLS_stage2.config

# ----------------------------------------------------------------------

# OK, off we go!

goto CFHTLS_STAGE1

# Results:

# Saturday - now including all stage 2 subjects!
# Also, set stage 1 end time to same as hasty (as used in defining stage 2 sample!)
# 2013-12-08_10:20:56
# ... and merged in Surhud's pull request, so results should be reproducible.


# SWAP: updating all subjects classified between the_beginning
# SWAP: and 2013-12-08_10:20:56
# SWAP: saving retiree subject Zooniverse IDs...
# SWAP: 420474 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_retire_these.txt
# SWAP: saving lens candidates...
# SWAP: 3367 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidates.txt
# SWAP: saving true positives...
# SWAP: 5306 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_true_positives.txt
# SWAP: saving false positives...
# SWAP: 4 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_false_positives.txt
# SWAP: saving false negatives...
# SWAP: 377 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_false_negatives.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 6590 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 5335 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 5 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_dud_catalog.txt

# Report says:
#    Number of classifications:    10768759
#    Number of classns used:       3705704
#    Number of classifiers:        36929
#    Number of test subjects:      427064
#    Number of sims:               5712
#    Number of duds:               4509
#    Mean test classns/classifier: 262.8
#    Mean classns/test subject:    8.4
#    Test subject retirements:     420474
#    Mean classns/retirement:      8.2
#    Test subject rejections:      420474
#    Test subject identifications: 3367
#    Lens completeness:            92.9%
#    Lens purity:                  0.1%
#    FP contamination:             99.9%
#    Lenses missed (FN rate):      6.6%



# And now stage 2. Try running in parallel?!

goto CFHTLS_STAGE2

# Results:

# SWAP: updating all subjects classified between 2013-12-10_13:29:41
# SWAP: and 2014-01-06_23:59:59
# SWAP: saving retiree subject Zooniverse IDs...
# SWAP: 2289 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_retire_these.txt
# SWAP: saving lens candidates...
# SWAP: 90 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_candidates.txt
# SWAP: saving true positives...
# SWAP: 119 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_training_true_positives.txt
# SWAP: saving false positives...
# SWAP: 1 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_training_false_positives.txt
# SWAP: saving false negatives...
# SWAP: 14 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_training_false_negatives.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 4032 subjects classified,
# SWAP: 1390 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 4032 subjects classified,
# SWAP: 138 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 4032 subjects classified,
# SWAP: 18 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_dud_catalog.txt

# Report says:
#   Number of classifications:    224745
#   Number of classns used:       224745
#   Number of classifiers:        1964
#   Number of test subjects:      3679
#   Number of sims:               152
#   Number of duds:               201
#   Mean test classns/classifier: 81.0
#   Mean classns/test subject:    43.2
#   Test subject retirements:     2289
#   Mean classns/retirement:      47.0
#   Test subject rejections:      2289
#   Test subject identifications: 90
#   Lens completeness:            78.3%
#   Lens purity:                  10.0%
#   FP contamination:             91.0%
#   Lenses missed (FN rate):      9.2%


# NB. I Forgot to set report = True in config files, this is now fixed.
# I ran SWAPSHOP again, with update.config edited to do reporting:

cd $SWAP_DIR/workspace/CFHTLS/stage1
SWAPSHOP.csh -s CFHTLS

cd $SWAP_DIR/workspace/CFHTLS/stage2
SWAPSHOP.csh -s CFHTLS --stage2

# 2014-06-01 (Sunday) 23:14 PDT

# Re-running stage 2 so that it agrees with Anu's run - with random state
# fixed. Number about to change again...

# ======================================================================
# 2014-05-30 (Friday) 17:50 PDT

# Make crowd plots, using stage 1 and stage 2 bureaus from above.

cd $SWAP_DIR/../doc/sw-system-figs/

set stage1bureau = $SWAP_DIR/workspace/CFHTLS/stage1/CFHTLS_bureau.pickle
set stage2bureau = $SWAP_DIR/workspace/CFHTLS/stage2/CFHTLS_bureau.pickle

make_crowd_plots.py $stage1bureau $stage2bureau

# Seem to work OK: tweaked labels etc, now write out numbers
# Stage 1 and 2 analysis is now done! Including tweak to make stage 1 end time
# equal the stage 2 start time.
# Numbers:

# make_crowd_plots: illustrating behaviour captured in bureau files:
# make_crowd_plots:  /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_bureau.pickle
# make_crowd_plots:  /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_bureau.pickle
# SWAP: read an old bureau of 36982 classification agents from /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_bureau.pickle
# SWAP: read an old bureau of 1964 classification agents from /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_bureau.pickle
# make_crowd_plots: stage 1, 2 agent numbers:  36982 1964
# make_crowd_plots:  782  volunteers stayed on for Stage 2 from Stage 1
#
# make_crowd_plots: mean stage 1 volunteer effort =  263.2
# make_crowd_plots: mean stage 1 volunteer experience =  28.9
# make_crowd_plots: mean stage 1 volunteer contribution =  34.9 bits
# make_crowd_plots: mean stage 1 volunteer skill =  0.04 bits
# make_crowd_plots: mean stage 2 volunteer effort =  81.0
# make_crowd_plots: mean stage 2 volunteer experience =  33.5
# make_crowd_plots: mean stage 2 volunteer contribution =  11.1 bits
# make_crowd_plots: mean stage 2 volunteer skill =  0.05 bits
#
# make_crowd_plots:  36982 stage 1 volunteers contributed 1292016.3 bits
# make_crowd_plots:  1.0 % of the volunteers - 375 people - contributed 90% of the information at Stage 1
# make_crowd_plots: total amount of information generated at stage 1 =  91122.6 bits
# make_crowd_plots:  9118 experienced stage 1 volunteers contributed 1290487.4 bits
# make_crowd_plots:  4.1 % of the experienced volunteers - 375 people - contributed 90% of the information at Stage 1
# make_crowd_plots:  1964 stage 2 volunteers contributed 21895.8 bits
# make_crowd_plots:  7.2 % of the volunteers - 141 people - contributed 90% of the information at Stage 2
# make_crowd_plots: total amount of information generated at stage 2 =  1640.4 bits
# make_crowd_plots: cumulative contribution plot saved to ./crowd_contrib_cumul.png
#
# make_crowd_plots:  36982 stage 1 volunteers possess 1471.9 bits worth of skill
# make_crowd_plots:  79.2 % of the skill possessed by the (20%) most skilled 7397 people
# make_crowd_plots:  9118 experienced stage 1 volunteers possess 1115.5 bits worth of skill
# make_crowd_plots:  43.1 % of the skill possessed by the (20%) most skilled 1824 people
# make_crowd_plots:  1964 stage 2 volunteers possess 102.4 bits worth of skill
# make_crowd_plots:  77.0 % of the skill possessed by the (20%) most skilled 393 people
# make_crowd_plots: cumulative skill plot saved to ./crowd_skill_cumul.png
#
# make_crowd_plots: the 649 - 7.0 % - of experienced stage 1 volunteers who have early skill > 0.1 go on to attain a mean final skill of 0.22
# make_crowd_plots: with 97.0 % of them remaining at skill 0.05 or higher
# make_crowd_plots: skill-skill plot saved to ./early_vs_final_skill.png
#
# make_crowd_plots: corner plot saved to ./all_skill_contribution_experience_education.png
#
# make_crowd_plots: total contribution in Stage 2 was 21895.8 bits by 1964 volunteers
# make_crowd_plots:  782 stage 1 veteran users ( 39.0 % of the total) made 94.0 % of the contribution
# make_crowd_plots: the average stage 1 veteran had skill, contribution, effort =  0.10 26.3 166
# make_crowd_plots:  1182 new users ( 60.0 % of the total) made 6.0 % of the contribution
# make_crowd_plots: the average stage 2 newbie had skill, contribution, effort =  0.02 1.1 24
# make_crowd_plots: newbies vs veterans plot saved to ./stage2_veteran_contribution.png
#
# make_crowd_plots: all done!


# ======================================================================
# 2014-06-01 (Sunday) 08:25 PDT

# Do some tidying up, to reclaim disk space:

mkdir -p $SWAP_DIR/projects/CFHTLS/stage1/results

mv stage1_results/* $SWAP_DIR/projects/CFHTLS/stage1/results/
\rm -rf stage1_results/

mkdir -p stage2/attic
mv stage2_phil/* stage2_phil/.swap.cookie stage2/attic/
\rm -rf stage2_phil

mkdir -p stage1/attic
mv allfuzzy fuzzy falsenegatives hasty knownlenses production talk stage1/attic/.

cd stage1/attic/
du -sch */*.pickle
\rm allfuzzy/CFHTLS_bureau.pickle allfuzzy/CFHTLS_collection.pickle fuzzy/CFHTLS_bureau.pickle fuzzy/CFHTLS_collection.pickle production/CFHTLS_bureau.pickle production/CFHTLS_collection.pickle

# OK - now, put results in projects/CFHTLS/stage?/results as we get them.
# Restart stage 1 SWAPSHOP...

# OK, do stage 1 candidates match those we actually injected into stage 2 interface?

wc -l CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidate*.txt \
      attic/hasty/CFHTLS_2013-12-08_10:20:56/CFHTLS_2013-12-08_10:20:56_candidate*.txt

# Today's run:
#     6591 CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidate_catalog.txt
#     3367 CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidates.txt

# The stage 1 run we actually used:
#     6310 attic/hasty/CFHTLS_2013-12-08_10:20:56/CFHTLS_2013-12-08_10:20:56_candidate_catalog.txt
#     3381 attic/hasty/CFHTLS_2013-12-08_10:20:56/CFHTLS_2013-12-08_10:20:56_candidates.txt

# Differences due to random seed? Have to use the old ones, I'm afraid.
# Crowd analysis has to use new ones though.

# Add more classifications? By changing end time in update.config?

# For now, rename "hasty" with "ACTUAL" and put in the stage1 workspace area
# for Anu. Check in.

# ======================================================================
# 2014-06-02 (Monday) 08:42 PDT

# Which has more classifications, ACTUAL or current? Check reports:

# ACTUAL/hasty: 10769187
# current:      10768759

# So current analysis stops short, by a few hundred...
# Use a more accurate end date?
# How about 2013-12-10_16:47:00 as previously thought?
# Or        2013-12-10_13:29:41 which is the  current stage 2 start time?
# Try the latter. Change update.config and run SWAPSHOP:

# Save a copy of the final update.config, ie as if we had done this:

# SWAPSHOP.csh -s CFHTLS --config CFHTLS_stage1_final_update.config

SWAPSHOP.csh -s CFHTLS

# Old report etc in CFHTLS_2013-09-17_18:36:31
# New report etc in CFHTLS_2013-12-08_10:20:57

# SWAP: total no. of classifications processed:  33366
# SWAP: saving agents to ./CFHTLS_bureau.pickle
# SWAP: saving subjects to ./CFHTLS_collection.pickle
# SWAP: saving retiree subject Zooniverse IDs...
# SWAP: 420475 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-12-08_10:20:57/CFHTLS_2013-12-08_10:20:57_retire_these.txt
# SWAP: saving lens candidates...
# SWAP: 3368 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-12-08_10:20:57/CFHTLS_2013-12-08_10:20:57_candidates.txt
# SWAP: saving true positives...
# SWAP: 5306 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-12-08_10:20:57/CFHTLS_2013-12-08_10:20:57_training_true_positives.txt
# SWAP: saving false positives...
# SWAP: 4 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-12-08_10:20:57/CFHTLS_2013-12-08_10:20:57_training_false_positives.txt
# SWAP: saving false negatives...
# SWAP: 378 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-12-08_10:20:57/CFHTLS_2013-12-08_10:20:57_training_false_negatives.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 6589 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-12-08_10:20:57/CFHTLS_2013-12-08_10:20:57_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 5334 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-12-08_10:20:57/CFHTLS_2013-12-08_10:20:57_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 5 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/CFHTLS_2013-12-08_10:20:57/CFHTLS_2013-12-08_10:20:57_dud_catalog.txt

# Report says:                     OLD          NEW ('-' denotes unchanged)
#    Number of classifications:    10768759		10802125
#    Number of classns used:       3705704		3705745
#    Number of classifiers:        36929		36982
#    Number of test subjects:      427064		-
#    Number of sims:               5712			-
#    Number of duds:               4509			4509 (?!)
#    Mean test classns/classifier: 262.8		263.2
#    Mean classns/test subject:    8.4			-
#    Test subject retirements:     420474		420475
#    Mean classns/retirement:      8.2			-
#    Test subject rejections:      420474		-
#    Test subject identifications: 3367			3368
#    Lens completeness:            92.9%		-
#    Lens purity:                  0.1%			-
#    FP contamination:             99.9%		-
#    Lenses missed (FN rate):      6.6%			-

# Almost no change in number of detections. New numbers are nice because of
# the stage 1/2 handover. Update config file in projects dir, tell Anu.

# ======================================================================
# 2014-06-02 (Monday) 08:42 PDT

# Stage1 analysis assumed PD,PL = 0.5 initially, conservatively.
# What if this had been 0.7 instead? More efficient? How would the FP
# and FN rates change with this optimism?
# Can also try unsupervised system too, more pessimistic.

# GENEROUS:

# Stage 1:

mkdirf $SWAP_DIR/workspace/CFHTLS/stage1/GENEROUS
cp $SWAP_DIR/projects/CFHTLS/stage1/CFHTLS_stage1.config CFHTLS_stage1_generous.config
# Edit to set PD,PL = 0.7
SWAPSHOP.csh -s CFHTLS --startup --config CFHTLS_stage1_generous.config

# Started 2014-06-02 (Monday) 09:53 PDT.....finished by 5pm.

# SWAP: saving retiree subject Zooniverse IDs...
# SWAP: 417264 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_retire_these.txt
# SWAP: saving lens candidates...
# SWAP: 3575 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidates.txt
# SWAP: saving true positives...
# SWAP: 5273 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_true_positives.txt
# SWAP: saving false positives...
# SWAP: 5 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_false_positives.txt
# SWAP: saving false negatives...
# SWAP: 411 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_false_negatives.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 9800 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 5301 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 6 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_dud_catalog.txt

# Report says:                    GENEROUS  STANDARD
# Number of classifications:      10802125  10802125
# Number of classns used:         3437025   3705745
# Number of classifiers:          36982     36982
# Number of test subjects:        427064    427064
# Number of sims:                 5712      5712
# Number of duds:                 4500      4509
# Mean test classns/classifier:   263.2     263.2
# Mean classns/test subject:      7.8       8.4
# Test subject retirements:       417264    420475
# Mean classns/retirement:        7.6       8.2
# Test subject rejections:        417264    420474
# Test subject identifications:   3575      3368
# Lens completeness:              92.3%     92.9%
# Lens purity:                    0.2%      0.1%
# FP contamination:               99.8%     99.9%
# Lenses missed (FN rate):        7.2%      6.6%

# Not big differences. More candidates, fewer false positives,
# higher false negative rate. 7% fewer classifications needed.


# Stage 2 (where candidates were selected by standard stage 1 analysis, of course...)

mkdirf $SWAP_DIR/workspace/CFHTLS/stage2/GENEROUS
cp $SWAP_DIR/projects/CFHTLS/stage1/CFHTLS_stage2.config CFHTLS_stage2_generous.config
# Edit to set PD,PL = 0.7
SWAPSHOP.csh -s CFHTLS --startup --config CFHTLS_stage2_generous.config

# SWAP: updating all subjects classified between 2013-12-10_13:29:41
# SWAP: and 2014-01-06_23:59:59
# SWAP: saving retiree subject Zooniverse IDs...
# SWAP: 2225 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_retire_these.txt
# SWAP: saving lens candidates...
# SWAP: 115 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_candidates.txt
# SWAP: saving true positives...
# SWAP: 116 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_training_true_positives.txt
# SWAP: saving false positives...
# SWAP: 0 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_training_false_positives.txt
# SWAP: saving false negatives...
# SWAP: 16 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_training_false_negatives.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 4032 subjects classified,
# SWAP: 1454 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 4032 subjects classified,
# SWAP: 136 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 4032 subjects classified,
# SWAP: 27 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_2013-12-10_16:47:03/CFHTLS_2013-12-10_16:47:03_dud_catalog.txt

# Report says:                    GENEROUS  STANDARD
# Number of classifications:      224745    224745
# Number of classns used:         224745    224745
# Number of classifiers:          1964      1964
# Number of test subjects:        3679      3679
# Number of sims:                 152       152
# Number of duds:                 201       201
# Mean test classns/classifier:   81.0      81.0
# Mean classns/test subject:      43.2      43.2
# Test subject retirements:       2225      2289
# Mean classns/retirement:        46.9      47.0
# Test subject rejections:        2225      2289
# Test subject identifications:   115       90
# Lens completeness:              76.3%     78.3%
# Lens purity:                    13.4%     10.0%
# FP contamination:               86.6%     91.0%
# Lenses missed (FN rate):        10.5%     9.2%

# Again, more candidates, but also more false negatives.


# How is crowd different?

cd $SWAP_DIR/workspace/CFHTLS/stage1/GENEROUS
set stage1bureau = $SWAP_DIR/workspace/CFHTLS/stage1/GENEROUS/CFHTLS_bureau.pickle
set stage2bureau = $SWAP_DIR/workspace/CFHTLS/stage2/GENEROUS/CFHTLS_bureau.pickle

make_crowd_plots.py $stage1bureau $stage2bureau

# make_crowd_plots: stage 1, 2 agent numbers:  36982 1964
# make_crowd_plots:  782  volunteers stayed on for Stage 2 from Stage 1
# make_crowd_plots: mean stage 1 volunteer effort =  263.2
# make_crowd_plots: mean stage 1 volunteer experience =  28.9
# make_crowd_plots: mean stage 1 volunteer contribution =  35.7 bits (cf 34.9 bits)
# make_crowd_plots: mean stage 1 volunteer skill =  0.10 bits (cf 0.04 bits)
# make_crowd_plots: mean stage 2 volunteer effort =  81.0
# make_crowd_plots: mean stage 2 volunteer experience =  33.5
# make_crowd_plots: mean stage 2 volunteer contribution =  14.4 bits (cf 11.1 bits)
# make_crowd_plots: mean stage 2 volunteer skill =  0.09 bits (cf 0.05 bits)
#
# make_crowd_plots:  36982 stage 1 volunteers contributed 1319322.0 bits (cf 1292016.3 bits)
# make_crowd_plots:  1.5 % of the volunteers - 541 people - contributed 90% of the information at Stage 1
#   (cf make_crowd_plots:  1.0 % of the volunteers - 375 people - contributed 90% of the information at Stage 1)
# make_crowd_plots: total amount of information generated at stage 1 =  103955.0 bits (cf 91122.6 bits)
# make_crowd_plots:  9118 experienced stage 1 volunteers contributed 1311399.6 bits
# make_crowd_plots:  5.9 % of the experienced volunteers - 541 people - contributed 90% of the information at Stage 1
# make_crowd_plots:  1964 stage 2 volunteers contributed 28371.3 bits (cf 21895.8 bits)
# make_crowd_plots:  10.8 % of the volunteers - 212 people - contributed 90% of the information at Stage 2
#   (cf make_crowd_plots:  7.2 % of the volunteers - 141 people - contributed 90% of the information at Stage 2)
# make_crowd_plots: total amount of information generated at stage 2 =  2269.1 bits (cf 1640.4 bits)
# make_crowd_plots: cumulative contribution plot saved to ./crowd_contrib_cumul.png
#
# make_crowd_plots:  36982 stage 1 volunteers possess 3786.8 bits worth of skill (cf 1471.9 bits)
# make_crowd_plots:  47.6 % of the skill possessed by the (20%) most skilled 7397 people
#   (cf make_crowd_plots:  79.2 % of the skill possessed by the (20%) most skilled 7397 people)
# make_crowd_plots:  9118 experienced stage 1 volunteers possess 1665.2 bits worth of skill
# make_crowd_plots:  19.4 % of the skill possessed by the (20%) most skilled 1824 people
# make_crowd_plots:  1964 stage 2 volunteers possess 181.9 bits worth of skill (cf 102.4 bits)
# make_crowd_plots:  59.5 % of the skill possessed by the (20%) most skilled 393 people
#    (cf make_crowd_plots:  77.0 % of the skill possessed by the (20%) most skilled 393 people)
# make_crowd_plots: cumulative skill plot saved to ./crowd_skill_cumul.png
#
# make_crowd_plots: the 5342 - 58.0 % - of experienced stage 1 volunteers who have early skill > 0.1 go on to attain a mean final skill of 0.24
# make_crowd_plots: with 98.0 % of them remaining at skill 0.05 or higher
# make_crowd_plots: skill-skill plot saved to ./early_vs_final_skill.png
#
# make_crowd_plots: corner plot saved to ./all_skill_contribution_experience_education.png
#
# make_crowd_plots: total contribution in Stage 2 was 28371.3 bits by 1964 volunteers
# make_crowd_plots:  782 stage 1 veteran users ( 39.0 % of the total) made 92.7 % of the contribution (cf 94.0%)
# make_crowd_plots: the average stage 1 veteran had skill, contribution, effort =  0.16 33.6 166 (cf 0.10 26.3 166)
# make_crowd_plots:  1182 new users ( 60.0 % of the total) made 7.3 % of the contribution (cf 6.0 %)
# make_crowd_plots: the average stage 2 newbie had skill, contribution, effort =  0.05 1.7 24 (cf 0.02 1.1 24)
# make_crowd_plots: newbies vs veterans plot saved to ./stage2_veteran_contribution.png


# ie not a big difference. Mean skill higher by fiat, but
# contribution numbers don't change much.

# ----------------------------------------------------------------------
# 2014-06-03 (Tuesday) 21:56 PDT

# UNSUPERVISED:

# Need to merge into unsupervised branch! Don't change branch until other test
# is done...
mkdirf $SWAP_DIR/workspace/CFHTLS/stage1/UNSUPERVISED
cp $SWAP_DIR/projects/CFHTLS/stage1/CFHTLS_stage1.config CFHTLS_stage1_unsupervised.config
# Edit to set unsupervised flag, and initial PD, PL to 0.7
SWAPSHOP.csh -s CFHTLS --startup --config CFHTLS_stage1_unsupervised.config

# Started Tuesday, 22:19 PDT
# Had to restart Weds morning as supervised keyword was not written out to update.config...
# Edited update.config:
SWAPSHOP.csh -s CFHTLS


# SWAP: saving retiree subject Zooniverse IDs...
# SWAP: 161621 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/UNSUPERVISED/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_retire_these.txt
# SWAP: saving lens candidates...
# SWAP: 3695 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/UNSUPERVISED/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidates.txt
# SWAP: saving true positives...
# SWAP: 5194 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/UNSUPERVISED/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_true_positives.txt
# SWAP: saving false positives...
# SWAP: 4 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/UNSUPERVISED/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_false_positives.txt
# SWAP: saving false negatives...
# SWAP: 444 lines written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/UNSUPERVISED/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_training_false_negatives.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 265443 candidates (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/UNSUPERVISED/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_candidate_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 5268 sim 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/UNSUPERVISED/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_sim_catalog.txt
# SWAP: saving catalog of high probability subjects...
# SWAP: From 437276 subjects classified,
# SWAP: 24 dud 'candidates' (with P > rejection) written to /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/UNSUPERVISED/CFHTLS_2013-09-17_18:36:31/CFHTLS_2013-09-17_18:36:31_dud_catalog.txt

# Report says:                  UNSUPERVISED  GENEROUS  STANDARD
# Number of classifications:      10802125    10802125  10802125
# Number of classns used:         3657057     3437025   3705745
# Number of classifiers:          36982       36982     36982
# Number of test subjects:        427064      427064    427064
# Number of sims:                 5712        5712      5712
# Number of duds:                 4500        4500      4509
# Mean test classns/classifier:   28.9        263.2     263.2
# Mean classns/test subject:      8.3         7.8       8.4
# Test subject retirements:       161621      417264    420475
# Mean classns/retirement:        9.1         7.6       8.2
# Test subject rejections:        161621      417264    420474
# Test subject identifications:   3695        3575      3368
# Lens completeness:              90.9%       92.3%     92.9%
# Lens purity:                    0.6%        0.2%      0.1%
# FP contamination:               99.4%       99.8%     99.9%
# Lenses missed (FN rate):        7.8%        7.2%      6.6%


# Higher FN rate than generous, which is higher than standard. Is this our primary metric?
# Unsupervised gives more FPs, but theres not much in it. Interesting.
# This is really "ignore training" because volunteers *have* been trained - still want training images in there!
# Try using both training and test images - shoudl be more efficient...
# Chris is looking into this.

# Merge in code from unsupervised branch! And make sure all checked in config
# files have correct flags...

# ======================================================================
# 2014-06-10 (Tuesday) 16:54 PDT

# Visual inspection of stage2 candidates. Go fast, no commenting!
# First some reorganisation.
# Make one master images directory, and keep app in visapp1, visapp2 etc
# folders. Then link to app from dated work directories.

# eg

cd $SWAP_DIR/workspace/CFHTLS/stage2/expert_inspection/2014-06-10/
cp File_list_newstage2 File_list
ln -s ../visapp2/visapp.jar .
ln -s ../visapp2/lib .
ln -s ../images .

# Now run the app:

java -jar visapp.jar &

# Follow progress:

tail -f record_position.txt

# Current state:
wc -l record_position.txt
#       42 record_position.txt

# Restarted, after install of java developer kit...
# 2014-06-27 (Friday) 15:35 BST

# Getting some "image not available" errors...
# - images did not contain all the needed pngs!
# - which ones were missed? Compare File_list with record_position.txt

sdiff -w 80 File_list_newstage2 junk | more

# Here are the ones I skipped...
# CFHTLS_023_2178_gri.png
# CFHTLS_120_0377_gri.png
# CFHTLS_071_1156_gri.png
# CFHTLS_024_2358_gri.png
# CFHTLS_146_1032_gri.png
# CFHTLS_157_1352_gri.png
# CFHTLS_221_2364_gri.png
# CFHTLS_139_1381_gri.png

# Paste them onto the bottom of the File_list!
# And make a copy.

wc -l File_list*
#      608 File_list
#      600 File_list_newstage2
#      608 File_list_with_skipped_ones_at_end

# 2014-07-10 (Thursday) 10:06 BST

# Carrying on - deadline day today!

# OK, all done! Made a copy of record_position.txt.
# Here's the breakdown:

# No of objects spotted:
cat  record_position_pjm_2014-07-10.txt | wc -l
     336
# No of images containing them:
cat record_position_pjm_2014-07-10.txt | awk '{print $1}' | sort | uniq | wc -l
     309
# No of grade 3 candidates:
cat record_position_pjm_2014-07-10.txt | awk '{if ($4 == 3) print $0}' | wc -l
      88
# No of grade 2 candidates:
cat record_position_pjm_2014-07-10.txt | awk '{if ($4 == 2) print $0}' | wc -l
      95
# No of grade 1 candidates:
cat record_position_pjm_2014-07-10.txt | awk '{if ($4 == 1) print $0}' | wc -l
     149

# Recall - all of this was done in /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/expert_inspection/2014-06-10

# ======================================================================
# 2014-06-12 (Thursday) 08:51 PDT

Pickles available on the web, for stage 1 and stage 2 analyses. cp from
workspace to be sure:

cd $SWAP_DIR/projects/CFHTLS/stage1/results/
cp /nfs/slac/g/ki/ki19/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/ACTUAL/CFHTLS_bureau.pickle .
cp /nfs/slac/g/ki/ki19/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/ACTUAL/CFHTLS_collection.pickle .

cd $SWAP_DIR/projects/CFHTLS/stage2/results/
cp /nfs/slac/g/ki/ki19/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_bureau.pickle .
cp /nfs/slac/g/ki/ki19/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_collection.pickle .

# These directories are not web-visible (in that the listings do not appear)
# but the files can be obtained by wget:

set baseurl = "http://www.slac.stanford.edu/~pjm"

cd $SWAP_DIR/projects/CFHTLS/stage1/results/
wget -N "${baseurl}/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/ACTUAL/CFHTLS_bureau.pickle"
wget -N "${baseurl}/SpaceWarps/Science/analysis/workspace/CFHTLS/stage1/ACTUAL/CFHTLS_collection.pickle"

cd $SWAP_DIR/projects/CFHTLS/stage2/results/
wget -N "${baseurl}/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_bureau.pickle"
wget -N "${baseurl}/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/CFHTLS_collection.pickle"

# The -N option turns on timestamping, at which point wget knows not
# to bother downloading if the files hasn't changed. The stage 1 pickles have
# December timestamps.

# ======================================================================
# 2014-06-25 (Wednesday) 12:17 BST

# Experiments with information gain for paper:
#
# 1) New subject, p0 = prior. Random classifier contributes zero information:
# >>> p0,M_LL, M_NN, C = 2e-4, 0.5, 0.5, True
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 0.0
#
# 2) New subject, p0 = prior. Low skill classifier contributes some information:
# >>> p0,M_LL, M_NN, C = 2e-4, 0.7, 0.7, True
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 0.000185630326368
#
# 2) New subject, p0 = prior. High skill classifier contributes information:
# >>> p0,M_LL, M_NN, C = 2e-4, 0.99, 0.99, True
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 0.100734717876
#
# 3) New subject, p0 = prior. Very high skill classifier contributes information:
# >>> p0,M_LL, M_NN, C = 2e-4, 0.999, 0.999, True
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 1.39706253249
#
# 4) New subject, p0 = prior. Perfect classifier contributes maximal information:
# >>> p0,M_LL, M_NN, C = 2e-4, 1, 1, True
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 12.2877123795
#
# 5) New subject, p0 = prior. Perfect classifier contributes some information
# going the other way (towards p=0):
# >>> p0,M_LL, M_NN, C = 2e-4, 1, 1, False
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 0.000288567865926
#
# 6) 50-50 subject, p0 = 0.5. Perfect classifier always contributes 1 bit:
# >>> p0,M_LL, M_NN, C = 0.5, 1, 1, False
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 1.0
# >>> p0,M_LL, M_NN, C = 0.5, 1, 1, True
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 1.0
#
# 7) New subject, p0 = prior. Obtuse classifier contributes same information
# once they are interpreted inversely:
# >>> p0,M_LL, M_NN, C = 2e-4, 0, 0, True
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 0.000288567865926
# >>> p0,M_LL, M_NN, C = 2e-4, 0, 0, False
# >>> print swap.informationGain(p0,M_LL,M_NN,C)
# 12.2877123795


# 2014-06-26 (Thursday) 07:38 BST

# Response from Surhud & Anu, formulate a reply:
#
# Hi Phil,
#
# Thanks for sending the new draft with comments. We were about to write
# to you because finally Anu and I had the chance last week to think more
# about the information contribution issue in SWAP. We started drafting
# this email before your email arrived, so forgive us if there is
# repetition between what you have written and what we say below.
#
# We must confess we are very new to information theory, so just looking
# at the code it was difficult to follow what you are doing exactly and
# where the equations you currently use come from. But looking at the
# plots and the descriptions in the text something seemed off...
#
# To start with we checked the plots made on the original github repo from
# which you imported the code in to spacewarps. We saw plots which were
# useful there but did not make sense. Let me share an example, according
# to your plots the information gained from an astute classifier
# classifying a low p0 candidate as lens is high and classifying something
# as a dud is small. For p0~2e-4, the information gained by an astute
# classifier classifying it as a lens is a whopping 12.x bits. And this
# keeps increasing at lower values of p0. While a dud classification for a
# subject from this classifier gives a very low information content 3e-4
# bits.
#
#   * The mutual information between the posterior and prior states is a
#   measure of information gain as a result of a classification. The more
#   different the posterior from the prior, ie the more surprising the
#   outcome, the bigger the information gain. So, regarding the
#   classification of low p0 subjects: it makes sense that the
#   classification of a low p0 subject as a lens should carry a larger
#   information gain than if it were classified as a dud.  For example, we
#   are already fairly sure that a subject with p0 = 2e-4 is NOT a lens,
#   and so a new classification that it's "NOT" brings very little
#   information. In contrast, the surprising classification that it's a
#   "LENS" carries a lot of information, provided the classifier is very
#   astute. This is the main reason why I wanted to investigate using the
#   mutual information to describe information flow in our system. It
#   seemed to me (and Edwin, in Taipei) to be the right quantity for
#   assessing the information transmitted by the agents.
#
#
# Consider two subjects, one was classified by an astute observer to be a
# lens, which implies a gain of ~12 bits. The second one was classified to
# be not a lens by another classifier who is not astute, thereby moving
# the value of p to smaller values. Subsequently this second subject was
# seen by an astute agent and classified as a lens. The total information
# gained from this lens could be much larger than ~12 bits... This is
# strange... The information gained should not depend upon the trajectory
# you take, but just on the final and initial conditions.
#
#   * I'm not sure that last sentence is correct. The mutual information I
#   am calculating is a property of a classification, not a subject. (I
#   think my own text could be confused on this point, though! I'll try
#   and fix this.) I suppose some subjects that oscillate around p~0.5
#   could be the cause of a total of more than 12 bits of information gain
#   as they are classified, as different low astuteness classifiers nudge
#   them around - but I'm not sure this is a fundamental problem, is it?
#
#
# We fundamentally disagree  with the notion that an astute classifier
# judging a subject as a lens on its first view gives 12 bits: this value
# should just be ~0.0003 bits. Here is why:
#
# The Shannon entropy of the subject tells us the uncertainty associated
# with it. This Shannon entropy is given by negative of the sum pi log pi,
# where pi is the probability of all possible states: in our case lens or
# not a lens. This value is about 3e-4 bits. When an astute observer
# classifies any subject to be a lens or not, this entropy vanishes,
# because the new p value for the subject becomes either 0 or 1, i.e., no
# uncertainty associated with the subject now. This loss of entropy is a
# result of gain of information about the true classification of the
# subject: and should be exactly equal to 3e-4 bits.
#
#   * It could be that quantifying the uncertainty of a given subject via
#   "its" entropy is just different from quantifying the information
#   transmitted by the agents. I'm interested to see what your plots show!
#
#
# I believe the reason why you claim a gain of information of ~12 bits for
# an astute classifier classifying a subject with p=2e-4 comes from the
# naive application of -log p, as the gain in information. This would have
# been correct if we knew there were datasets of subjects say of 5000
# images, and each of the datasets was known to contain exactly one lens.
# In this case, as soon as you find the lens in one of the
# classifications, you know the state of the rest of the images, and thus
# it is reasonable that you gain ~12 bits of information. But this clearly
# does not correspond to our case.
#
#   * I'm not sure I buy this argument. One can compute the mutual
#   information between posterior and prior states for a project
#   containing a single subject, and the formulae would be just the same:
#   they apply to each classification independently (as you pointed out in
#   one of your other objections above). I agree that the 12 bits number
#   looks slightly strange at first, but the same formula gives the very
#   intuitive result that the information generated by a perfectly astute
#   classifier when classifying a p0=0.5 subject is 1 bit. Given this,
#   it's clear that the more surprising classification of a p0=2e-4
#   subject has to carry more than 1 bit of information.
#
#
# So last week, we coded up what we believe is the right information
# gain/loss (yes information is lost when a classifier moves the
# probability of a subject from 2e-4 to anywhere less than 0.5). We think
# these new routines should be used in place of the current shannon.py.
# Have a look at the fork here:
#
# https://github.com/anupreeta27/SpaceWarps
#
# We are in the process of rerunning the Stage 1 and Stage 2 analyses, and
# making the plots again. We will report back on this tomorrow during the
# hangout.
#
#   * OK! I will try and get hold of Edwin and figure out the meaning of
#   what you have coded. I see you haven't latexed the derivation of this
#   from first principles - do you have this, by any chance? I know this
#   request is a bit rich coming from me! (Although Edwin and I did write
#   a wiki page on our workings in March.)
#
# Cheers, Surhud


# This was followed by up by some debugging, of both shannon.py and
# shannonnew.py. Looks like Edwin changed the informationgain function
# in some way, not sure why! Need to ask him. ExpectedInformationGain()
# looks OK though.

# 2014-06-30 (Monday) 16:14 BST

# OK: my notes, and the OldI in informationgain.py, and Surhud and Anu's
# code, had informationGain = entropyChange, not cross entropy (KL
# divergence). This is the right one to use, and is indeed what Edwin
# had coded (just hadnt explained to me). Change latex to match!

# ======================================================================
# 2014-07-22 (Tuesday) 12:06 BST

# Query from Anu about some expert inspections:
#
#     My rank 3 which received rank 1 from either or both of u:
#     CFHTLS_055_1537
#     CFHTLS_106_0865
#     CFHTLS_213_1078
#     CFHTLS_108_1088
#     Some are difficult to note doubles but I was biased to rank them higher because I knew these systems already (these are known sl2s lenses). Would you like to update your ranks ?
#
#     Your rank 3 and mine rank 1:
#     For all of the below, I gave less weight because of the low curvature of the arcs (some of these could be easily rank 2 from my perspective, so i can switch to that)
#     CFHTLS_126_0352
#     CFHTLS_142_1003
#     CFHTLS_338_1728

# Anu's grade 3:
open \
images/CFHTLS_055_1537_gri.png \
images/CFHTLS_106_0865_gri.png \
images/CFHTLS_213_1078_gri.png \
images/CFHTLS_108_1088_gri.png

CFHTLS_055_1537 - tangential alignment, counter not obvious, image sep high: 2
CFHTLS_106_0865 - bright arc, good curvature, just no visible counter: 2
CFHTLS_213_1078 - no obvious multiple imaging, but clearly a lot of mass: 2
CFHTLS_108_1088 - seems high image separation? could be group, maybe: 2


# Anu's grade 1:
open \
images/CFHTLS_126_0352_gri.png \
images/CFHTLS_142_1003_gri.png \
images/CFHTLS_338_1728_gri.png

CFHTLS_126_0352 - long faint arc, some curvature, counter obscured by stars? 2
CFHTLS_142_1003 - long brght arc, some curvature, but wide im sep: 1
CFHTLS_338_1728 - faint long arc, but straight. No visible counter: 2



# Check my own good candidates, all have P > 0.95:
http://talk.spacewarps.org/#/subjects/ASW0007xrs   # Commenters disagree
http://talk.spacewarps.org/#/subjects/ASW0007h27   # 2 possible lenses
http://talk.spacewarps.org/#/subjects/ASW0005o38   # Nice galaxy-scale arc
http://talk.spacewarps.org/#/subjects/ASW0004pbz   # Tiny binary lens
http://talk.spacewarps.org/#/subjects/ASW0004nan   # Bright UV arc under LRG
http://talk.spacewarps.org/#/subjects/ASW00008a0   # Thin arcs, multiple imaging
http://talk.spacewarps.org/#/subjects/ASW0009bbq   # Faint cluster arc
http://talk.spacewarps.org/#/subjects/ASW00096rm   # Nice tiny quad
http://talk.spacewarps.org/#/subjects/ASW000993q   # Cluster lens, plus galaxy pertuber
http://talk.spacewarps.org/#/subjects/ASW0006i2r   # Nice arc next to LRG
http://talk.spacewarps.org/#/subjects/ASW0004106   # Sheared arc! 3-plane lens :-)
http://talk.spacewarps.org/#/subjects/ASW0005vog   # Binary lens, faint arc with knot

set candidates = ( \
ASW0007xrs \
ASW0007h27 \
ASW0005o38 \
ASW0004pbz \
ASW0004nan \
ASW00008a0 \
ASW0009bbq \
ASW00096rm \
ASW000993q \
ASW0006i2r \
ASW0004106 \
ASW0005vog \
)
set cat =  projects/CFHTLS/stage2/results/CFHTLS_2013-12-10_16:47:03_candidate_catalog.txt

foreach candidate ( $candidates )
  grep $candidate $cat
end

# All are high P, well classified. Don't have CFHTLS ids.
# ======================================================================
# 2014-07-30 (Wednesday) 18:22 PDT

# Further query from Anu re expert classifications:
#
#      R>2.5: 47 subjects (coln 5); might change a bit after revised ranks; Also, NB:
#      these are subjects not lenses, again subject to change.
#
#      systems where expert gave R>2.5 but SW gave P<0.02
#      CFHTLS_102_0883 - Galaxy scale system, with counter-image! 25
#                        classifications, P = 0.012
#      CFHTLS_308_0224 - Faint arc around bright pair of BGGs. Disappointing!
#                        After 49 classifications, P = 0.005
#      CFHTLS_100_2061 - Arc in top right hand corner. Was it picked up at
#                        higher P in the other three images it's in?
#                        21 classifications, P = 0.003

# PJM annotations added.

#      Please revise ranks for:
#      ================
#      CFHTLS_165_1602 - phil ??
#      CFHTLS_040_0534 - phil ?? double classf
#      CFHTLS_131_0568 - av ?
#      CFHTLS_109_1055/56 - av different grades ?
#      CFHTLS_120_0377 -- phil?  humvi artefact ? (see fits below)
#      http://www2.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/cadcbin/megapipe/imc.pl?lang=en&object=&ra=215.04267&dec=52.674838&size=128
#
# CFHTLS_165_1602 Nice bright blue arc, counter-image too - but it's a merger of
#                 two fairly isolated galaxies, so a) it could be merger-induced
#                 star formation or satellite disruption, and b) the image
#                 separation seems high for a still relatively low mass
#                 environment, no? So I gave it a "possibly". I guess I could be
#                 persuaded to give it a "probably" given its quite a massive
#                 galaxy: 2
#
# CFHTLS_040_0534 I initially thought 1, then changed my mind: I think this
#                 purple arc is probably a lensed feature. 2
#
# CFHTLS_120_0377 Oops yes - looks like a spiral in the Megapipe images. HumVI
#                 artifact, perhaps due to variable seeing? 1
#
# ======================================================================
# 2014-08-05 (Tuesday) 09:25 PDT

# Checking more expert grades, via spreadsheet online.

SPREADCHECK:

cd ~/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/expert_inspection/images/

set IDs = (\
CFHTLS_204_0633 \
CFHTLS_073_2191 \
CFHTLS_153_1112 \
CFHTLS_146_1032 \
CFHTLS_139_1381 \
CFHTLS_139_1380 \
CFHTLS_119_2273 \
CFHTLS_171_1278 \
CFHTLS_142_0979 \
CFHTLS_262_1714 \
CFHTLS_204_0843 \
CFHTLS_165_1602 \
CFHTLS_109_2391 \
CFHTLS_304_0397 \
CFHTLS_060_0352 \
CFHTLS_130_1154 \
CFHTLS_074_2382 \
CFHTLS_058_1933 \
CFHTLS_159_0285 \
CFHTLS_130_2180 \
CFHTLS_147_1730 \
CFHTLS_138_1245 \
CFHTLS_041_1919 \
CFHTLS_057_0809 \
CFHTLS_060_1970 \
CFHTLS_164_0190 \
CFHTLS_017_0018 \
CFHTLS_074_2146 \
CFHTLS_105_1198 \
CFHTLS_138_2240 \
CFHTLS_077_1207 \
CFHTLS_006_0783 \
CFHTLS_114_1679 \
CFHTLS_112_2019 \
CFHTLS_337_0790 \
CFHTLS_306_1184 \
CFHTLS_049_0030 \
CFHTLS_238_1046 \
CFHTLS_285_1089 \
CFHTLS_025_0088 \
CFHTLS_028_2334 \
CFHTLS_019_2462 \
CFHTLS_067_1823 \
CFHTLS_076_1510 \
CFHTLS_160_1516 \
CFHTLS_085_1826 \
CFHTLS_136_1881 \
CFHTLS_110_2454 \
CFHTLS_073_1229 \
CFHTLS_030_1392 \
CFHTLS_050_1186 \
CFHTLS_304_1590 \
CFHTLS_082_2096 \
CFHTLS_033_1965 \
CFHTLS_248_1415 \
CFHTLS_137_1492 \
CFHTLS_046_0355 \
CFHTLS_137_1332 \
CFHTLS_113_2034 \
CFHTLS_062_2308 \
CFHTLS_118_0948 \
CFHTLS_112_0072 \
CFHTLS_139_1906 \
CFHTLS_016_0396 \
CFHTLS_003_2056 \
CFHTLS_041_2241 \
CFHTLS_058_1846 \
CFHTLS_070_1087 \
CFHTLS_212_2176 \
CFHTLS_025_2368 \
CFHTLS_086_1132 \
CFHTLS_089_0310 \
CFHTLS_018_0755 \
CFHTLS_073_1571 \
CFHTLS_142_1003 \
CFHTLS_046_2010 \
CFHTLS_020_1912 \
CFHTLS_092_2321 \
CFHTLS_056_1368 \
CFHTLS_102_0541 \
CFHTLS_028_0574 \
CFHTLS_153_1857 \
CFHTLS_069_1132 \
CFHTLS_148_2344 \
CFHTLS_092_1080 \
CFHTLS_062_0845 \
CFHTLS_291_1246 \
CFHTLS_092_0241 \
CFHTLS_298_1770 \
CFHTLS_070_2218 \
CFHTLS_334_0482 \
CFHTLS_291_1719 \
CFHTLS_092_1080 \
CFHTLS_066_0290 \
CFHTLS_168_1027 \
CFHTLS_213_1078 \
CFHTLS_082_0469 \
CFHTLS_063_0807 \
CFHTLS_101_2282 \
CFHTLS_005_1296 \
)

set pngs = ()
foreach ID ( $IDs )
    set pngs = ( $pngs ${ID}_gri.png )
end

open $pngs

goto FINISH

# ======================================================================
# 2014-08-05 (Tuesday) 16:40 PDT

# PNG cutouts from Chris. Hi simages are super-sampled!
# No wonder they're big. Maybe he needed them big to make the contours look
# nice?

# gallery up:

cd /Users/pjm/public_html/SpaceWarps/Science/analysis/workspace/CFHTLS/stage2/atlas

set IDs = `ls *png | cut -d'_' -f1-3 | sort -n | uniq`
set pngs = ()
foreach ID ( $IDs )
    set pngs = ( $pngs ${ID}_field.png ${ID}_field_output.png ${ID}_cluster*png )
end

gallery.pl -x 3 -y 3 -pdf -o atlas.pdf $pngs

# Hmm - new version has multiple clusters in it! How to display? With
# one row per image, and blanks in some columns if there are few clusters...

# ======================================================================
# 2014-08-08 (Friday) 05:52 PDT

# From Anupreeta:
#
#
#     Here are some notes/to-do items before our telcon this Friday.
#
#     1. Discuss these along with the remaining spread=2
#     =========================
#     CFHTLS_017_0018 - same as CFHTLS_025_2368
#     CFHTLS_106_1051 - same as CFHTLS_105_1198
#     CFHTLS_109_2391
#     CFHTLS_073_1571  upgrade further? blue arc+counter ??
#     CFHTLS_092_1080 (see attached png and fits files) -- none are interesting acc. to me
#     CFHTLS_062_0845 (see attached fits) near the center at 2'o clock - looks unlikely
#     CFHTLS_092_0546 (upgraded upon inspection)
#
#     If possible, please do not sort the google doc until Friday's discussion.
#
#     2. I have attached a tar with catalogs of candidates with R_avg =
#     2.3,2.0,1.7,1.3,1.0. You could load them in the applet one by one or use
#     your fav. viewer to have a quick look at them with the aim of answering
#     the following question:
#
#     ** What subset should be shown in the figs and what subset should be presented
#     in the Table (cutoff at 1.0 or 2.0 or in between )? **
#
#     Please note these numbers (ie., no. of subjects; will change after removing
#     duplicates):
#
#       Category      N_each   N_cum
#     ==================================
#         R_avg>2.5     50        50    N_unique~35  Known~30
#         R_avg_2.3     30        80
#         R_avg_2.0     30       110
#         R_avg_1.7     39       149
#         R_avg_1.3     25       174
#         R_avg_1.0     23       197
#
#
#     My assessment after doing the above inspection is that R_avg>=1.3 look good
#     enough for reporting in the Table format. I believe that after removing the
#     known ones and repeats, R_avg>=1.3 might be good enough to even show in the
#     figs. Otherwise, we could go to 1.7 but certainly not below 2.0.
#
#     3. How to compare and present Pvalues with the expert grades ?
#
#

open \
CFHTLS_017_0018_gri.png \  nearby galaxy with stars? very white "lens": 1
CFHTLS_106_1051_gri.png \  nice candidate, though fluffy: 2/3
CFHTLS_109_2391_gri.png \  wide sep but arc/counter: 3
CFHTLS_073_1571_gri.png \  merging galaxy, but arc/counter: 2
CFHTLS_092_1080_gri.png \  which object? lots of mass. 2?
CFHTLS_062_0845_gri.png \  no counter but nice blue arc. 3
CFHTLS_092_0546_gri.png    which object? lots of mass. 2?

# Aprajita said:
#
# CFHTLS_017_0018 - same as CFHTLS_025_2368
# looks like satellites to me but can upgrade if you 2 are confident
#
# CFHTLS_106_1051 - same as CFHTLS_105_1198
# spread=1 but OK, please make mine consistent with 105_1198 (upgrade 2)
#
# CFHTLS_109_2391
# now has spread=1, v faint arc, blue blobs too far out?
#
# CFHTLS_073_1571  upgrade further? blue arc+counter ??
# spread=1 possible - too hard to tell, can upgrade to 2 if u 2  re confident
#
# CFHTLS_092_1080 (see attached png and fits files) -- none are interesting acc. to me
# possible arc but bit far, but thick and funny orientation
#
# CFHTLS_062_0845 (see attached fits) near the center at 2'o clock - looks unlikely
# v thin arc? downgrade to 1
#  what about the blue thing not he green trail? 232 so ok
#
# CFHTLS_092_0546 (upgraded upon inspection)
# spread=0, all good?


# Anu says there are 88 +/- few subjects contining
# known lenses in the 600 we inspected. About 100 didn't make it to stage 2.

# Post-telecon:
#
#     We went through the rest of the 1.7, we upgraded a couple of yours from 1->2
#     to the ones we were most confident on, there were a few that we debated that
#     you might like to look at

open \
      CFHTLS_147_1730_gri.png \
      CFHTLS_138_1245_gri.png \
      CFHTLS_057_0809_gri.png

# ======================================================================
# 2014-08-25 (Monday) 21:34 PDT

# Look at galleries in paper 2, check we are happy with grades of each one
# Names on gallery panels don't match lines in spreadsheet!!!!

# OK, download tarball from email ionto candidates folder and view in preview
# there, to get names right. Just looking for promotions and demotions.

cd analysis/workspace/CFHTLS/stage2/candidates

# Start with R >= 2.5 panels: demote any?

open lenscand_2.5.pdf
# 024_2215 Edge on, 2.3
# 142_0979 Double, 2.3

open lenscand_2.3.pdf
# 073_2191 Unconvincing arc? 1.7
# 062_0845 "" 1.7
# 169_0592 Obvious giant arcs?! 2.5
# 106_1051 Arc and counter arc, 2.5
# 060_0352 Possible compound lens, but not too much curvature... 2.0

open lenscand_2.0.pdf
# Demote to 1.7:
# 089_0310 no counter-image?
# 092_0049 not enough curvature
# 092_0546
# 119_2273 singly imaged
# 131_0357 ring galaxy
# 131_2432 spiral? cant tell
# 249_0265 to bright to see arcs
# promote to 2.3
# 147_1730 binary lens, bright arcs

open lenscand_1.7.pdf
# promote to 2.0
# 066_0290 nice curvature across disk?
# demote to 1.3
# 027_2325 ring galaxy!

open lenscand_1.3.pdf
# 159_0285 binary lens, not much fuzz apart from blue arcs, 1.7
# 142_1003 no counter image but good arc definition, 1.7
# 131_0568 could be Einstein ring? 1.7


# Quick look through offline_extras - see a few 1-2's, plus:
# 075_0005  red arcs, 2.5
# 099_0477  massive cluster, thin red high z arc? 2.3
# 156_0012  straightish arc through group 2.3
# 168_1300  straight arc between BGGs 2.3


OK, visual inspection of offline extras:

cd offline_extras
ls -l *png | awk '{print $NF}' > offline_extras_file_list
wc -l offline_extras_file_list
# 238

# Now rearrange, we need a working directory and these images
# in an "images" directory:

cd ../
mv images online

# and th elist in the File_list:

mkdirf 2014-08-27
ln -s ../offline_extra images
mv offline_extras_file_list File_list
ln -s ../visapp2/visapp.jar .

# OK, now can do inspection:

java -jar visapp.jar &

# Follow progress:

tail -f record_position.txt


# 2014-09-02 (Tuesday) 14:16 PDT

# Inspecting offline deiscrepant systems:

open \
CFHTLS_169_1303_gri.png \
CFHTLS_019_0882_gri.png \
CFHTLS_168_1300_gri.png \
CFHTLS_132_1679_gri.png \
CFHTLS_122_2397_gri.png \
CFHTLS_114_1574_gri.png \
CFHTLS_091_0819_gri.png \
CFHTLS_089_2040_gri.png \
CFHTLS_089_2039_gri.png \
CFHTLS_076_1953_gri.png \
CFHTLS_022_0671_gri.png \
CFHTLS_015_0346_gri.png \
CFHTLS_068_2401_gri.png \
CFHTLS_059_0098_gri.png \
CFHTLS_164_1649_gri.png \
CFHTLS_140_1121_gri.png \
CFHTLS_103_0406_gri.png \
CFHTLS_085_1183_gri.png \
CFHTLS_077_1920_gri.png \
CFHTLS_067_2054_gri.png \
CFHTLS_062_0311_gri.png \
CFHTLS_057_2380_gri.png \
CFHTLS_048_0767_gri.png \
CFHTLS_029_0688_gri.png

# ======================================================================
# 2014-09-03 (Wednesday) 15:02 PDT

# Plotting specific subjects' trajectories. Notes/code from Anu:
#
# 1. Using the attached file (loc_cfhtid*), you can extract ids,cfhtids of those
# subjects which you want to plot trajectories of and store them in
# "interesting_candidates.txt"
#
# 2. In SWAP.py where you plot Subject trajectories, you can add these lines:
#
#  ids,cfhtids=np.loadtxt("interesting_candidates.txt",dtype='string',unpack=1);
#
#          for ID in ids:
#              try:
#                 sample.member[ID].plot_trajectory2(fig3);
#                 print "SWAPDBG: Found ",ID," and plotting, it was ",sample.member[ID].status,sample.member[ID].kind
#              except KeyError:
#                 print "SWAPDBG: Notfound ",ID," and continuing, it was not_found"
#                 continue;
#
#
# 3. In analysis/swap/subject.py, You can add the plot_trajectory2 function (see
# below) which will plot the magenta trajectories.
#
#
#     def plot_trajectory2(self,axes):
#
#         plt.sca(axes[0])
#         N = np.linspace(0, len(self.trajectory)/Ntrajectory+1, len(self.trajectory)/Ntrajectory, endpoint=True);
#         N[0] = 0.5
#         mdn_trajectory=np.array([]);
#         sigma_trajectory_m=np.array([]);
#         sigma_trajectory_p=np.array([]);
#         for i in range(len(N)):
#         sorted_arr=np.sort(self.trajectory[i*Ntrajectory:(i+1)*Ntrajectory])
#             sigma_p=sorted_arr[int(0.84*Ntrajectory)]-sorted_arr[int(0.50*Ntrajectory)]
#             sigma_m=sorted_arr[int(0.50*Ntrajectory)]-sorted_arr[int(0.16*Ntrajectory)]
#             mdn_trajectory=np.append(mdn_trajectory,sorted_arr[int(0.50*Ntrajectory)]);
#             sigma_trajectory_p=np.append(sigma_trajectory_p,sigma_p);
#             sigma_trajectory_m=np.append(sigma_trajectory_m,sigma_m);
#
#         if self.kind == 'sim':
#             colour = 'blue'
#         elif self.kind == 'dud':
#             colour = 'red'
#         elif self.kind == 'test':
#         # this is different
#             colour = 'magenta'
#
#         if self.status == 'undecided':
#             facecolour = colour
#         else:
#             facecolour = 'white'
#
#         # this is different
#         plt.plot(mdn_trajectory,N,color=colour,alpha=0.15,linewidth=1.0, linestyle="-")
#
#         NN = N[-1]
#         if NN > swap.Ncmax: NN = swap.Ncmax
#         plt.scatter(mdn_trajectory[-1], NN, edgecolors=colour, facecolors=facecolour, alpha=0.5);
#         plt.plot([mdn_trajectory[-1]-sigma_trajectory_m[-1],mdn_trajectory[-1]+sigma_trajectory_p[-1]],[NN,NN],color=colour,alpha=0.3);
#         # if self.kind == 'sim': print self.trajectory[-1], N[-1]
#
#         return


# So, need to update subject.py to plot highlighted trajectories (not
# necessarily in magenta!), and then write a standalone trajectory plotter
# (better than having SWAP do it, I think - its too specialized a task).

mkdirf  $SWAP_DIR/workspace/CFHTLS/trajectories

set collection2 = $SWAP_DIR/workspace/CFHTLS/stage2/CFHTLS_collection.pickle

set FNcatalog2 = $SWAP_DIR/projects/CFHTLS/stage2/results/online/CFHTLS_2013-12-10_16:47:03_sim_catalog.txt
set FNlist2 = ${FNcatalog:r}_P.lt.0.0001.txt
set FNpng2 = CFHTLS_stage2_false-negatives_trajectories.png

# ZooIDs:
# grep -v '#' $FNcatalog2 | awk '{if ($2 < 0.0001) print $1}' > $FNlist2

# Database IDs:
grep -v '#' $FNcatalog2 | awk '{if ($2 < 0.0001) print $4}' | \
  cut -d'/' -f6 | cut -d'.' -f1 > $FNlist2
wc -l $FNlist2
# 6

make_trajectory_plots.py -f $FNlist2 -t "Stage 2 False Negatives" $collection2
mv trajectories.png $FNpng2

# OK! Got trajectories, no histograms, on linear scale, for 6 false negatives.

# Now do false positives:

set FPcatalog2 = $SWAP_DIR/projects/CFHTLS/stage2/results/online/CFHTLS_2013-12-10_16:47:03_dud_catalog.txt
set FPlist2 = ${FPcatalog2:r}_P.gt.0.95.txt
set FPpng2 = CFHTLS_stage2_false-positives_trajectories.png

# Database IDs:
grep -v '#' $FPcatalog2 | awk '{if ($2 > 0.95) print $4}' | \
  cut -d'/' -f6 | cut -d'.' -f1 > $FPlist2
wc -l $FPlist2
# 1

make_trajectory_plots.py -f $FPlist2 -t "Stage 2 False Positives" $collection2
mv trajectories.png $FPpng2

# Looks to me like its not the experts making mistakes - but just noise from the random walk.

# Check the stage 1 systems too:

set collection1 = $SWAP_DIR/workspace/CFHTLS/stage1/CFHTLS_collection.pickle

set FNcatalog1 = $SWAP_DIR/projects/CFHTLS/stage1/results/CFHTLS_2013-12-08_10:20:56_sim_catalog.txt
set FNlist1 = ${FNcatalog:r}_P.lt.0.00001.txt
set FNpng1 = CFHTLS_stage1_false-negatives_trajectories.png

grep -v '#' $FNcatalog1 | awk '{if ($2 < 0.00001) print $4}' | \
  cut -d'/' -f6 | cut -d'.' -f1 > $FNlist1
wc -l $FNlist1
# 4

make_trajectory_plots.py -f $FNlist1 -t "Stage 1 False Negatives" $collection
mv trajectories.png $FNpng1

# Now do false positives:

set FPcatalog1 = $SWAP_DIR/projects/CFHTLS/stage1/results/CFHTLS_2013-12-08_10:20:56_dud_catalog.txt
set FPlist1 = ${FPcatalog1:r}_P.gt.0.95.txt
set FPpng1 = CFHTLS_stage1_false-positives_trajectories.png

grep -v '#' $FPcatalog1 | awk '{if ($2 > 0.95) print $4}' | \
  cut -d'/' -f6 | cut -d'.' -f1 > $FPlist1
wc -l $FPlist1
# 4

make_trajectory_plots.py -f $FPlist1 -t "Stage 1 False Positives" $collection
mv trajectories.png $FPpng1

# OK, FPs show a few costly mistakes...
# Maybe because PD is less well constrained at stage 1?

# OK, look at trajectories next to subjects.
# Need to pull down PNGs and save next to personal trajectory plot.
# Plot title needs to be "ASW0000XXX: Stage 1 False Positive" etc,
# and filenames should be ASW0000XXX_gri.png, ASW0000XXX_stage1_trajectory.png

FNFPTRAJECTORIES:

set collections = ( $collection1 $collection2 )
set FNcatalogs = ( $FNcatalog1 $FNcatalog2 )
set FPcatalogs = ( $FPcatalog1 $FPcatalog2 )
set FNlists = ( $FNlist1 $FNlist2 )
set FPlists = ( $FPlist1 $FPlist2 )

set manifest = loc_cfhtid_d1_d11

set panels = ( )

foreach stage ( 1 2 )
  set collection = $collections[$stage]
  foreach failure ( 'FN' 'FP' )
    if ($failure == 'FN') then
      set catalog = $FNcatalogs[$stage]
      set list = $FNlists[$stage]
      set titleending = ": Stage $stage False Negative"
    else
      set catalog = $FPcatalogs[$stage]
      set list = $FPlists[$stage]
      set titleending = ": Stage $stage False Positive"
    endif
    foreach ID ( `cat $list` )
      set ZooID = `grep $ID $catalog | awk '{print $1}'`
      set title = "${ZooID}$titleending"
      set url = `grep $ID $catalog | awk '{print $4}'`
      set image = ${ZooID}_gri.png
      set plot = ${ZooID}_stage${stage}_trajectory.png
      if (! -e $plot) then
        wget -O $image "$url"
        echo $ID > toto
        set log = $plot:r.log
        make_trajectory_plots.py -f toto -t "$title" $collection >& $log
        mv trajectories.png $plot
      endif
      set panels = ( $panels $image )
      set panels = ( $panels $plot )
      # du -h $image $plot
      set CFHTLSID = `grep $ID $manifest | awk '{print $2}'`
      echo "$title  ( $CFHTLSID )"
    end
  end
end

# gallery.pl -x 2 -y 2 -pdf -o trajectories_gallery.pdf $panels

goto FINISH

# OK, send around. FNs are genuinely hard!
# Do see some costly mistakes, in stage 1 FPs, eg ASW0003j2s (and also ASW0002c4t,
# which is the same object, weirdly).

# ======================================================================
# 2014-09-27 (Saturday) 12:11 PDT

# Visual inspection of extra 60 offline extras:

mkdirf 2014-09-27
ln -s ../offline_extra_60 images
mv offline_extras_60/File_list File_list
ln -s ../visapp2/visapp.jar .

# OK, now can do inspection:

java -jar visapp.jar &

# Follow progress:

tail -f record_position.txt

# Done.
# Had to copy some images over from the online tarball.
# 2 files were missing:
#   CFHTLS_165_1845_gri.png
#   CFHTLS_211_1224_gri.png

# Sent record_position_pjm.txt to Anu.

# 2014-10-02 (Thursday) 16:45 PDT

# From Anu:
# Here are the discrepant systems. The order is AV, PJM and AM
#
# CFHTLS_003_1593_gri.png 40.0 202.0     2 1 0  too far apart + merger?
# CFHTLS_022_2021_gri.png 154.66 170.66  2 0 0
# CFHTLS_127_1932_gri.png 95.0 21.0      2 3 0  merger?
# CFHTLS_139_1034_gri.png 334.0 244.0    2 0 0
# CFHTLS_145_0148_gri.png 179.0 273.0    2 0 0
# CFHTLS_146_0251_gri.png 235.0 210.0    2 0 0
# CFHTLS_165_1845_gri.png 118.0 411.0    2 0 0  missing for Phil
# CFHTLS_211_1224_gri.png 62.0 403.0     2 0 1  missing for Phil
# CFHTLS_042_1284_gri.png 265.91 291.52  0 2 0  merger?

set conflicts = ( \
images/CFHTLS_003_1593_gri.png \  # 1
images/CFHTLS_022_2021_gri.png \  # 1 missed this the first time...
images/CFHTLS_127_1932_gri.png \  # 1 maybe merger. arcs not that round...
images/CFHTLS_139_1034_gri.png \  # 0 arc too far away
images/CFHTLS_145_0148_gri.png \  # 0
images/CFHTLS_146_0251_gri.png \  # 0
images/CFHTLS_165_1845_gri.png \  # 1 maybe something in bottom left?
images/CFHTLS_211_1224_gri.png \  # 1
images/CFHTLS_042_1284_gri.png \  # 1 ok, could be a merger, arc not quite right...
)

open $conflicts

# New grades are above as comments...

# ======================================================================
# 2014-11-06 (Thursday) 14:33 PST

# HumVI settings for SW DES:

# Downloaded data from Anu:

cd /Users/pjm/public_html/SpaceWarps/DES
wget http://member.ipmu.jp/anupreeta.more/des_cutouts_1.tgz
tar xvfz des_cutouts_1.tgz

set IDs = DES_1106641609684


foreach ID ( $IDs )

  compose.py -v -s 1.0,1.2,2.5 -z 0.0 -p 1.0,0.03 -m -1.0 \
    -o ${ID}_gri.png \
    ${ID}_i.fits ${ID}_r.fits ${ID}_g.fits \

end




# ======================================================================

FINISH:
